{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "oos-cluster-other-atis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "        \n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib as plt\n",
        "import json\n",
        "%matplotlib inline\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2022-01-22T17:23:00.950822Z",
          "iopub.execute_input": "2022-01-22T17:23:00.951148Z",
          "iopub.status.idle": "2022-01-22T17:23:01.653052Z",
          "shell.execute_reply.started": "2022-01-22T17:23:00.951063Z",
          "shell.execute_reply": "2022-01-22T17:23:01.652256Z"
        },
        "trusted": true,
        "id": "tZVpAUQH2lg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "! pip install sentence-transformers\n",
        "!pip install lexrank\n",
        "import numpy as np\n",
        "from sentence_transformers import *\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "import matplotlib.cm as cm\n",
        "import torch\n",
        "import sentence_transformers\n",
        "#from lexrank_utility import *\n",
        "import umap\n",
        "import plotly\n",
        "plotly.offline.init_notebook_mode (connected = True)\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "#import glob\n",
        "import json\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn.metrics import silhouette_score\n",
        "import scipy.cluster.hierarchy as shc\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.utils import shuffle\n",
        "from numpy.random import randn, uniform\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler\n",
        "\n",
        "!pip install git+https://github.com/boudinfl/pke.git\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "import re\n",
        "import pke\n",
        "\n",
        "!pip install git+https://github.com/arvkevi/kneed.git\n",
        "!pip install openpyxl\n",
        "! pip install scikit-learn-extra\n",
        "! pip install pyclustering\n",
        "!pip install faiss\n",
        "! pip install faiss-cpu --no-cache\n",
        "import faiss"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-22T17:23:01.654602Z",
          "iopub.execute_input": "2022-01-22T17:23:01.654930Z"
        },
        "trusted": true,
        "id": "RZwE07OK2lg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
        "from tensorflow.keras.layers import Embedding,concatenate\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "%matplotlib inline\n",
        "\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import torch\n",
        "import shutil\n",
        "import string\n",
        "import nltk.data\n",
        "import matplotlib\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM,SpatialDropout1D,Reshape,RNN\n",
        "from tensorflow.keras.layers import Embedding,concatenate\n",
        "from tensorflow.keras.layers import Conv2D, GlobalMaxPooling2D,MaxPool2D,MaxPool3D,GlobalAveragePooling2D,Conv3D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "import datetime\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import time, datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import datetime as dt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans,MiniBatchKMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "from kneed import DataGenerator, KneeLocator\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.utils import shuffle\n",
        "from numpy.random import randn, uniform\n",
        "from scipy.stats import norm\n",
        "from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.preprocessing import StandardScaler, normalize\n",
        "from sklearn.metrics import silhouette_score\n",
        "import scipy.cluster.hierarchy as shc\n",
        "from sklearn.cluster import Birch,DBSCAN,MeanShift,SpectralClustering\n",
        "from sklearn.mixture import GaussianMixture\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import os\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.cluster import AffinityPropagation\n",
        "import re\n",
        "import pke\n",
        "pip install keybert[all]\n",
        "from keybert import KeyBERT\n",
        "import spacy \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.cluster import MiniBatchKMeans\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "-nictAi08uRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('../input/atis-airlinetravelinformationsystem/atis_intents.csv')\n",
        "data_train=pd.read_csv('../input/atis-snips/atis_snips/snips/train.csv')\n",
        "data_test=pd.read_csv('../input/atis-snips/atis_snips/snips/test.csv')"
      ],
      "metadata": {
        "trusted": true,
        "id": "XkcShBbt2lg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test"
      ],
      "metadata": {
        "trusted": true,
        "id": "HiF_-f9-2lhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train=data_train.rename(columns={'intent':\"tag\",'text':'title'})\n",
        "data_test=data_test.rename(columns={'intent':\"tag\",'text':'title'})\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "t78c98BJ2lhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['tag'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "0fFMF8YA2lhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_test['tag'].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "74YMLUVn2lhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=data_train.copy()\n",
        "df1=data_test.copy()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Hil_H_AV2lhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dd={'SearchScreeningEvent':\"oos\",'GetWeather':\"oos\"}\n",
        "#dd={'SearchScreeningEvent':\"oos\",'GetWeather':\"oos\",'BookRestaurant':'oos'}\n",
        "#dd={'SearchScreeningEvent':\"oos\",'GetWeather':\"oos\",'BookRestaurant':'oos','PlayMusic':'oos','RateBook':'oos'}"
      ],
      "metadata": {
        "trusted": true,
        "id": "MyHRgVKh2lhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1_copy=df1.copy()  ##Test class copy()\n",
        "df1_copy['tag'].replace(dd,inplace=True)  ####preparing testdataset so making the unseen classes oos\n",
        "data_test_with_oos=df1_copy "
      ],
      "metadata": {
        "trusted": true,
        "id": "jYle_6jB2lhL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_test_with_oos['tag'].unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "5aal7jwo2lhM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexNames = df1_copy[(df1_copy['tag'] == 'oos')].index\n",
        "# Delete these row indexes from dataFrame\n",
        "\n",
        "data_test_without_oos=df1_copy.drop(indexNames)  ###to make data_test_without_oos we drop the oos classses from df1_copy\n",
        "print(data_test_without_oos['tag'].value_counts())"
      ],
      "metadata": {
        "trusted": true,
        "id": "YzxZkU-C2lhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "65PGlglX2lhQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy=df.copy()## making a copy of training dataset\n",
        "df_copy['tag'].replace(dd,inplace=True)  ##the traininfg dataset must be free of oos class so first marking unseen classes as oos\n",
        "#print(df_copy['tag'].unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "I_Sxx0IA2lhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexNames = df_copy[(df_copy['tag'] == 'oos')].index\n",
        "# Delete these row indexes from dataFrame\n",
        "\n",
        "data_train=df_copy.drop(indexNames)   ##dropping the oos classes from training dataset\n",
        "data_train"
      ],
      "metadata": {
        "trusted": true,
        "id": "GGsFA49e2lhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RESETTING THE INDEX OF ALL\n",
        "data_train.reset_index(inplace=True,drop=True)\n",
        "data_test_without_oos.reset_index(inplace=True,drop=True)\n",
        "data_test_with_oos.reset_index(inplace=True,drop=True)\n",
        "print('Training datalength->',len(data_train),\"Test data with oos->\",len(data_test_with_oos),\"Test data without oos->\",len(data_test_without_oos))"
      ],
      "metadata": {
        "trusted": true,
        "id": "H8qXgfLX2lhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['title']=data_train['title'].astype(str)\n",
        "data_test_without_oos['title']=data_test_without_oos['title'].astype(str)\n",
        "data_test_with_oos['title']=data_test_with_oos['title'].astype(str)"
      ],
      "metadata": {
        "trusted": true,
        "id": "5h6b7Sxu2lhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk.data \n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle') \n",
        "# Loading PunktSentenceTokenizer using English pickle file \n",
        "\n",
        "\n",
        "\n",
        "def no_of_words_in_paragraph(x):\n",
        "    return len(x)\n",
        "\n",
        "data_train['no_of_words_in_paragraph']=data_train['title'].apply(lambda x:no_of_words_in_paragraph(x))\n",
        "\n",
        "data_test_without_oos['no_of_words_in_paragraph']=data_test_without_oos['title'].apply(lambda x:no_of_words_in_paragraph(x))\n",
        "data_test_with_oos['no_of_words_in_paragraph']=data_test_with_oos['title'].apply(lambda x:no_of_words_in_paragraph(x))\n",
        "\n",
        "\n",
        "print(data_train)\n",
        "avg=data_train['no_of_words_in_paragraph'].mean()\n",
        "maxim=data_train['no_of_words_in_paragraph'].max()\n",
        "print('average paragraph length',data_train['no_of_words_in_paragraph'].mean())\n",
        "print('maximum para length',data_train['no_of_words_in_paragraph'].max())\n",
        "print('hii')\n",
        "excess=(data_train['no_of_words_in_paragraph']>avg).sum()\n",
        "excess_ratio=excess/len(data_train)\n",
        "print('excess_ratio',excess_ratio)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_sent_token(x):\n",
        "    return tokenizer.tokenize(x) \n",
        "#converting each paragraph into separate sentences\n",
        "\n",
        "data_train['sentence_token']=data_train['title'].apply(lambda x: make_sent_token(x))\n",
        "\n",
        "data_test_without_oos['sentence_token']=data_test_without_oos['title'].apply(lambda x: make_sent_token(x))\n",
        "\n",
        "data_test_with_oos['sentence_token']=data_test_with_oos['title'].apply(lambda x: make_sent_token(x))\n",
        "\n",
        "\n",
        "# In[99]:\n",
        "\n",
        "\n",
        "#data_train\n",
        "\n",
        "\n",
        "# In[100]:\n",
        "\n",
        "\n",
        "data_train['no_of_sentences']=data_train['sentence_token'].apply(lambda x:len(x))\n",
        "\n",
        "\n",
        "# In[101]:\n",
        "\n",
        "\n",
        "data_test_without_oos['no_of_sentences']=data_test_without_oos['sentence_token'].apply(lambda x:len(x))\n",
        "data_test_with_oos['no_of_sentences']=data_test_with_oos['sentence_token'].apply(lambda x:len(x))\n",
        "\n",
        "\n",
        "avg_sen_length=data_train['no_of_words_in_paragraph'].sum()/data_train['no_of_sentences'].sum()\n",
        "print(avg_sen_length)"
      ],
      "metadata": {
        "trusted": true,
        "id": "kexoY1mY2lhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def max_length_of_sentence(x,y):\n",
        "    sen=x\n",
        "    nu=y\n",
        "    #print(sen)\n",
        "    ma=0\n",
        "    if(nu>1):\n",
        "        l=sen.split('.')\n",
        "        #print(l)\n",
        "        for i in range(len(l)):\n",
        "            k=l[i].replace(',','')\n",
        "            maxi=len(k.split())\n",
        "            #print(maxi)\n",
        "            if(maxi>ma):\n",
        "                ma=maxi\n",
        "        return ma\n",
        "    else:\n",
        "        return len(sen.split())\n",
        "        \n",
        "    \n",
        "\n",
        "\n",
        "# In[106]:\n",
        "\n",
        "\n",
        "data_train['max_words_in_sentence']=data_train.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)\n",
        "\n",
        "data_test_without_oos['max_words_in_sentence']=data_test_without_oos.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)\n",
        "data_test_with_oos['max_words_in_sentence']=data_test_with_oos.apply(lambda x: max_length_of_sentence(x.title,x.no_of_sentences),axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RLLY9ARw2lhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x1=max(data_train['no_of_sentences'])\n",
        "y1=max(data_train['max_words_in_sentence'])\n",
        "\n",
        "x2=max(data_test_with_oos['no_of_sentences'])\n",
        "y2=max(data_test_with_oos['max_words_in_sentence'])\n",
        "\n",
        "if(x1>=x2):\n",
        "    m=x1\n",
        "    print(\"original m\",m)\n",
        "    m=m\n",
        "    print(\"m//4\")\n",
        "else:\n",
        "    m=x2\n",
        "    print(\"original m\",m)\n",
        "    m=m\n",
        "    print(\"m//4\")\n",
        "    \n",
        "if(y1>=y2):\n",
        "    n=y1\n",
        "else:\n",
        "    n=y2\n",
        "\n",
        "#So each para will be converted to a m*n matrix\n",
        "if(m<2):\n",
        "    m=6\n",
        "else:\n",
        "    m+=5\n",
        "print('x1,x2,y1,y2',x1,x2,y1,y2)\n",
        "#So each para will be converted to a m*n matrix\n",
        "print('final m selected',m,n)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T17:49:42.540165Z",
          "iopub.execute_input": "2021-07-21T17:49:42.540523Z",
          "iopub.status.idle": "2021-07-21T17:49:42.552623Z",
          "shell.execute_reply.started": "2021-07-21T17:49:42.540489Z",
          "shell.execute_reply": "2021-07-21T17:49:42.551646Z"
        },
        "trusted": true,
        "id": "dUBMQSfG2lhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "import string \n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "def make_tokens(text):     ##Converting into single tokens in order to create the vocabulary\n",
        "    return word_tokenize(text)\n",
        "\n",
        "\n",
        "data_train['tokens']=data_train['title'].apply(lambda x: make_tokens(x))\n",
        "data_test_without_oos['tokens']=data_test_without_oos['title'].apply(lambda x: make_tokens(x))\n",
        "data_test_with_oos['tokens']=data_test_with_oos['title'].apply(lambda x: make_tokens(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T17:49:47.628849Z",
          "iopub.execute_input": "2021-07-21T17:49:47.629238Z",
          "iopub.status.idle": "2021-07-21T17:49:49.071283Z",
          "shell.execute_reply.started": "2021-07-21T17:49:47.629204Z",
          "shell.execute_reply": "2021-07-21T17:49:49.070298Z"
        },
        "trusted": true,
        "id": "QxDq3vdU2lhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_index = {}\n",
        "f = open('../input/glove6b300dtxt/glove.6B.300d.txt')\n",
        "for line in f:\n",
        "    values = line.split(' ')\n",
        "    word = values[0] ## The first entry is the word\n",
        "    coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('GloVe data loaded')\n",
        "\n",
        "## More code adapted from the keras reference (https://github.com/keras-team/keras/blob/master/examples/pretrained_word_embeddings.py)\n",
        "# prepare embedding matrix \n",
        "\n",
        "\n",
        "\n",
        "all_training_words = [word for tokens in data_train[\"tokens\"] for word in tokens]\n",
        "training_sentence_lengths = [len(tokens) for tokens in data_train[\"tokens\"]]\n",
        "TRAINING_VOCAB = sorted(list(set(all_training_words)))\n",
        "print(\"%s words total, with a vocabulary size of %s\" % (len(all_training_words), len(TRAINING_VOCAB)))\n",
        "print(\"Max sentence length is %s\" % max(training_sentence_lengths))\n",
        "para_max=max(training_sentence_lengths)\n",
        "\n",
        "vocab=len(TRAINING_VOCAB)\n",
        "# In[114]:\n",
        "\n",
        "\n",
        "#len(TRAINING_VOCAB)\n",
        "\n",
        "\n",
        "# In[115]:\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "tokenizer = Tokenizer(num_words=len(TRAINING_VOCAB), char_level=False)\n",
        "tokenizer.fit_on_texts(data_train['title'])       # we assigned values \n",
        "\n",
        "\n",
        "# In[116]:\n",
        "\n",
        "\n",
        "train_word_index = tokenizer.word_index\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def make_train_seq(x):\n",
        "    return tokenizer.texts_to_sequences(x)\n",
        "data_train['train_seq']=data_train['sentence_token'].apply(lambda x:make_train_seq(x) )\n",
        "data_test_without_oos['train_seq']=data_test_without_oos['sentence_token'].apply(lambda x:make_train_seq(x) )\n",
        "data_test_with_oos['train_seq']=data_test_with_oos['sentence_token'].apply(lambda x:make_train_seq(x) )\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "def padding(x):    #now padding each sentence to a length of n...number of columns\n",
        "    MAX_SENTENCE_LENGTH=n  #(no of columns)\n",
        "    return pad_sequences(x,maxlen=MAX_SENTENCE_LENGTH,padding='post')\n",
        "\n",
        "data_train['padded']=data_train['train_seq'].apply(lambda x:padding(x))\n",
        "data_test_without_oos['padded']=data_test_without_oos['train_seq'].apply(lambda x:padding(x))\n",
        "data_test_with_oos['padded']=data_test_with_oos['train_seq'].apply(lambda x:padding(x))\n",
        "\n",
        "\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.initializers import Constant\n",
        "\n",
        "## EMBEDDING_DIM =  ## seems to need to match the embeddings_index dimension\n",
        "EMBEDDING_DIM = embeddings_index.get('a').shape[0]\n",
        "print(EMBEDDING_DIM)\n",
        "#num_words = min(MAX_NUM_WORDS, len(word_index)) + 1\n",
        " #= np.zeros(len(train_word_index) + 1, EMBEDDING_DIM)\n",
        "train_embedding_weights = np.zeros((len(train_word_index)+1, \n",
        " EMBEDDING_DIM))\n",
        "for word, i in train_word_index.items():\n",
        "    #print(\"sd\")\n",
        "    embedding_vector = embeddings_index.get(word) ## This references the loaded embeddings dictionary\n",
        "    if embedding_vector is not None:\n",
        "        train_embedding_weights[i] = embedding_vector\n",
        "print(train_embedding_weights.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T17:49:49.073235Z",
          "iopub.execute_input": "2021-07-21T17:49:49.07364Z",
          "iopub.status.idle": "2021-07-21T17:50:38.28672Z",
          "shell.execute_reply.started": "2021-07-21T17:49:49.073597Z",
          "shell.execute_reply": "2021-07-21T17:50:38.285901Z"
        },
        "trusted": true,
        "id": "sh8q7U8v2lhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_full_para(x):     #92 cross 192 matrix of a paragraph.   (m*n)\n",
        "    l=len(x)\n",
        "    h=m-l#no. of extra rows to be added\n",
        "    if(h>0):\n",
        "        z=[0]*h*n       #1D vector(#addding extra lines for zeroes as padding)\n",
        "        z=np.reshape(z,(h,n))    #reshaping it to match the dimension of paragraph\n",
        "        s=x.tolist()+z.tolist()\n",
        "        return s \n",
        "    else:\n",
        "        return x.tolist()[:m]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data_train['full_para']=data_train['padded'].apply(lambda x : make_full_para(x))\n",
        "data_test_without_oos['full_para']=data_test_without_oos['padded'].apply(lambda x : make_full_para(x))\n",
        "data_test_with_oos['full_para']=data_test_with_oos['padded'].apply(lambda x : make_full_para(x))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T17:50:38.289819Z",
          "iopub.execute_input": "2021-07-21T17:50:38.290086Z",
          "iopub.status.idle": "2021-07-21T17:50:38.780958Z",
          "shell.execute_reply.started": "2021-07-21T17:50:38.290059Z",
          "shell.execute_reply": "2021-07-21T17:50:38.780041Z"
        },
        "trusted": true,
        "id": "U6yTGxs42lhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_1d_para(x):\n",
        "    l=[]\n",
        "    for i in x:\n",
        "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
        "    return l\n",
        "        \n",
        "data_train['single_d_array']=data_train['full_para'].apply(lambda x: create_1d_para(x) )\n",
        "data_test_without_oos['single_d_array']=data_test_without_oos['full_para'].apply(lambda x: create_1d_para(x) )\n",
        "data_test_with_oos['single_d_array']=data_test_with_oos['full_para'].apply(lambda x: create_1d_para(x) )\n",
        "\n",
        "\n",
        "# In[128]:\n",
        "\n",
        "\n",
        "train_cnn_data=np.array(data_train['single_d_array'].tolist())\n",
        "\n",
        "\n",
        "# In[129]:\n",
        "\n",
        "\n",
        "p_without_oos=np.array(data_test_without_oos['single_d_array'].tolist())\n",
        "p_with_oos=np.array(data_test_with_oos['single_d_array'].tolist())\n",
        "\n",
        "\n",
        "test_cnn_data_without_oos=p_without_oos\n",
        "\n",
        "test_cnn_data_with_oos=p_with_oos\n",
        "\n",
        "\n",
        "label_names=data_train['tag'].unique()\n",
        "new=pd.get_dummies(data_train['tag'])\n",
        "label_names.tolist()\n",
        "y_train=new[label_names].values\n",
        "\n",
        "\n",
        "\n",
        "print(label_names)\n",
        "classes=len(label_names)\n",
        "\n",
        "print(data_train['tag'])\n",
        "\n",
        "for i in range(classes):\n",
        "    ass=(data_train['tag']==label_names[i]).sum()\n",
        "    print(label_names[i],ass)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T17:50:38.782333Z",
          "iopub.execute_input": "2021-07-21T17:50:38.782667Z",
          "iopub.status.idle": "2021-07-21T17:50:39.251863Z",
          "shell.execute_reply.started": "2021-07-21T17:50:38.782628Z",
          "shell.execute_reply": "2021-07-21T17:50:39.250272Z"
        },
        "trusted": true,
        "id": "injZC-gJ2lhX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOTA"
      ],
      "metadata": {
        "id": "U55dCj012lhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "####SVM\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_te=make_embeddings(data_test_without_oos,'normal-message')\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn import svm\n",
        "lin_clf = svm.LinearSVC()\n",
        "calibrated_clf = CalibratedClassifierCV(base_estimator=lin_clf)\n",
        "calibrated_clf.fit(X_tr,data_train['tag'])\n",
        "dec = calibrated_clf.predict_proba(X_te)\n",
        "label_names.sort()\n",
        "output_class_pred=[]\n",
        "for i in range(len(dec)):\n",
        "    ind=np.argmax(dec[i])\n",
        "    output_class_pred.append(label_names[ind])\n",
        "\n",
        "original_ans=data_test_without_oos['tag'].tolist()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "def multiclass_metrics_without_oos(cnf_matrix):\n",
        "    \n",
        "\t#print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\tclosed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('closed_accuracy',closed_accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(closed_accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"Exceeding_ratio\":excess_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "resi_without_oos=multiclass_metrics_without_oos(cnf_matrix)\n",
        "resi_without_oos.to_csv('SVMresults_text_without_oos.csv', mode='w', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)\n",
        "\n",
        "\n",
        "#########################\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_te=make_embeddings(data_test_with_oos,'normal-message')\n",
        "\n",
        "\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn import svm\n",
        "lin_clf = svm.LinearSVC(C=5)\n",
        "calibrated_clf = CalibratedClassifierCV(base_estimator=lin_clf)\n",
        "calibrated_clf.fit(X_tr,data_train['tag'])\n",
        "dec = calibrated_clf.predict_proba(X_te)\n",
        "label_names.sort()\n",
        "output_class_pred=[]\n",
        "for i in range(len(dec)):\n",
        "    ind=np.argmax(dec[i])\n",
        "    output_class_pred.append(label_names[ind])\n",
        "    \n",
        "    \n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "def check_metric_with_oos_static_thresholding(original_ans):\n",
        "    output_class_pred=[]\n",
        "    #y_test=y_test\n",
        "    open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "    pre=[]\n",
        "    recall=[]\n",
        "    f1=[]\n",
        "    specificity=[]\n",
        "    sensitivity=[]\n",
        "    GMean1=[]\n",
        "    Gmean2=[]\n",
        "    MCC=[]\n",
        "    bin_acc=[]\n",
        "    '''for i in range(len(y_test)):\n",
        "        ma=max(y_test[i])\n",
        "        output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "    #print(y_test)\n",
        "    print(output_class_pred)\n",
        "    '''\n",
        "    \n",
        "    lin_clf = svm.LinearSVC(C=5)\n",
        "    calibrated_clf = CalibratedClassifierCV(base_estimator=lin_clf)\n",
        "    calibrated_clf.fit(X_tr,data_train['tag'])\n",
        "    dec = calibrated_clf.predict_proba(X_te)\n",
        "    label_names.sort()\n",
        "    #output_class_pred=[]\n",
        "    for i in range(len(dec)):\n",
        "        ind=np.argmax(dec[i])\n",
        "        ma=np.max(dec[i])\n",
        "        output_class_pred.append([label_names[ind],ma])\n",
        "        \n",
        "    \n",
        "                                 \n",
        "    y_test=output_class_pred\n",
        "    \n",
        "        \n",
        "    for t in [0.6,0.7,0.8,0.9]: \n",
        "        for i in range(len(output_class_pred)):\n",
        "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                output_class_pred[i][0]=\"oos\"\n",
        "        #print(output_class_pred)\n",
        "        \n",
        "        rightly_predicted_count=0\n",
        "        for i in range(len(output_class_pred)):\n",
        "            if( output_class_pred[i][0]==original_ans[i]):\n",
        "                rightly_predicted_count+=1\n",
        "\n",
        "        #accuracy=rightly_predicted_count/len(y_test)\n",
        "        print('open_accuracy',rightly_predicted_count/len(output_class_pred))\n",
        "        open_accuracy=rightly_predicted_count/len(output_class_pred)\n",
        "        df=[]\n",
        "        TP=0\n",
        "        FP=0\n",
        "        TN=0\n",
        "        FN=0\n",
        "        for i in range(len(y_test)):\n",
        "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                FP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                TP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                FN+=1\n",
        "        \n",
        "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                TN+=1\n",
        "        print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "        binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "    \n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print(\"binary_accuracy\",binary_acc)\n",
        "        print('precision_oos:' ,TP/(TP+FP))\n",
        "        print('recall_oos:',TP/(FN+TP))\n",
        "        print(\"F1_oos:\",F1)\n",
        "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        #open_acc.append(open_accuracy)\n",
        "        bin_acc.append(binary_acc)\n",
        "        open_acc.append(open_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        \n",
        "    matrix=[0.6,0.7,0.8,0.9]\n",
        "    print(len(matrix))\n",
        "    data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "    metric=pd.DataFrame(data)\n",
        "    return metric,df\n",
        "original_ans=data_test_with_oos['tag'].tolist()\n",
        "resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(original_ans)\n",
        "resi_with_oos_static_threshold.to_csv('SVMresults_text_with_oos_static_threshold.csv', mode='w', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)\n",
        "\n",
        "######################\n",
        "\n",
        "############L2AC\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_te=make_embeddings(data_test_without_oos,'normal-message')\n",
        "\n",
        "\n",
        "simi=cosine_similarity(X_te,X_tr)\n",
        "output_class_pred=[]\n",
        "for i in range(len(simi)):\n",
        "    ma=np.argmax(simi[i])\n",
        "    output_class_pred.append(data_train['tag'][ma])\n",
        "    \n",
        "original_ans=data_test_without_oos['tag'].tolist()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "def multiclass_metrics_without_oos(cnf_matrix):\n",
        "    \n",
        "\t#print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\tclosed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('closed_accuracy',closed_accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(closed_accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"Exceeding_ratio\":excess_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "resi_without_oos=multiclass_metrics_without_oos(cnf_matrix)\n",
        "resi_without_oos.to_csv('L2ACresults_text_without_oos.csv', mode='w', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)\n",
        "######################\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_te=make_embeddings(data_test_with_oos,'normal-message')\n",
        "\n",
        "\n",
        "simi=cosine_similarity(X_te,X_tr)\n",
        "pred_with_oos=[]\n",
        "\n",
        "'''for i in range(len(simi)):\n",
        "    ma=np.max(simi[i])\n",
        "    ind=np.argmax(simi[i])\n",
        "    if(ma>0.5):\n",
        "        output_class_pred.append(data_train['tag'][ind])\n",
        "    else:\n",
        "        output_class_pred.append('oos')\n",
        "'''\n",
        "        \n",
        "    \n",
        "#original_ans=data_test_without_oos['tag'].tolist()\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "def check_metric_with_oos_static_thresholding(original_ans):\n",
        "    output_class_pred=[]\n",
        "    #y_test=y_test\n",
        "    open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "    pre=[]\n",
        "    recall=[]\n",
        "    f1=[]\n",
        "    specificity=[]\n",
        "    sensitivity=[]\n",
        "    GMean1=[]\n",
        "    Gmean2=[]\n",
        "    MCC=[]\n",
        "    bin_acc=[]\n",
        "    '''for i in range(len(y_test)):\n",
        "        ma=max(y_test[i])\n",
        "        output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "    #print(y_test)\n",
        "    print(output_class_pred)\n",
        "    '''\n",
        "    simi=cosine_similarity(X_te,X_tr)\n",
        "    pred_with_oos=[]\n",
        "\n",
        "    for i in range(len(simi)):\n",
        "        ma=np.max(simi[i])\n",
        "        ind=np.argmax(simi[i])\n",
        "        output_class_pred.append([data_train['tag'][ind],ma])\n",
        "        \n",
        "    print()\n",
        "    y_test=output_class_pred\n",
        "    \n",
        "        \n",
        "    for t in [0.5,0.6,0.7,0.8,0.9]: \n",
        "        for i in range(len(output_class_pred)):\n",
        "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                output_class_pred[i][0]=\"oos\"\n",
        "        #print(output_class_pred)\n",
        "        \n",
        "        rightly_predicted_count=0\n",
        "        for i in range(len(output_class_pred)):\n",
        "            if( output_class_pred[i][0]==original_ans[i]):\n",
        "                rightly_predicted_count+=1\n",
        "\n",
        "        #accuracy=rightly_predicted_count/len(y_test)\n",
        "        print('open_accuracy',rightly_predicted_count/len(output_class_pred))\n",
        "        open_accuracy=rightly_predicted_count/len(output_class_pred)\n",
        "        df=[]\n",
        "        TP=0\n",
        "        FP=0\n",
        "        TN=0\n",
        "        FN=0\n",
        "        for i in range(len(y_test)):\n",
        "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                FP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                TP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                FN+=1\n",
        "        \n",
        "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                TN+=1\n",
        "        print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "        binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "    \n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print(\"binary_accuracy\",binary_acc)\n",
        "        print('precision_oos:' ,TP/(TP+FP))\n",
        "        print('recall_oos:',TP/(FN+TP))\n",
        "        print(\"F1_oos:\",F1)\n",
        "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        #open_acc.append(open_accuracy)\n",
        "        bin_acc.append(binary_acc)\n",
        "        open_acc.append(open_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        \n",
        "    matrix=[0.5,0.6,0.7,0.8,0.9]\n",
        "    print(len(matrix))\n",
        "    data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "    metric=pd.DataFrame(data)\n",
        "    return metric,df\n",
        "original_ans=data_test_with_oos['tag'].tolist()\n",
        "resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(original_ans)\n",
        "resi_with_oos_static_threshold.to_csv('L@ACresults_text_with_oos_static_threshold.csv', mode='w', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)\n",
        "\n",
        "###############\n",
        "###############YOONKIM----DOCLSTM\n",
        "\n",
        "def create_single_line_para(x):\n",
        "    l=[]\n",
        "    for i in x:\n",
        "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
        "    return l\n",
        "        \n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "data_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_without_oos['create_single_line_para']=data_test_without_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_with_oos['create_single_line_para']=data_test_with_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "\n",
        "# In[ ]:\n",
        "print('aas')\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "yoon_kim_train_data=np.array(data_train['create_single_line_para'])\n",
        "yoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')\n",
        "print('dsd')\n",
        "# In[ ]:\n",
        "yoon_kim_test_data=np.array(data_test_without_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_without_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "print('sdsd')\n",
        "\n",
        "yoon_kim_test_data=np.array(data_test_with_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_with_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "\n",
        "print('vsv')\n",
        "\n",
        "#from __future__ import print_function\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
        "from tensorflow.keras.layers import Embedding,concatenate\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "label_names=data_train['tag'].unique()\n",
        "new=pd.get_dummies(data_train['tag'])\n",
        "label_names.tolist()\n",
        "y_train=new[label_names].values\n",
        "\n",
        "\n",
        "# In[133]:\n",
        "\n",
        "\n",
        "print(label_names)\n",
        "classes=len(label_names)\n",
        "\n",
        "print(data_train['tag'])\n",
        "\n",
        "for i in range(classes):\n",
        "    ass=(data_train['tag']==label_names[i]).sum()\n",
        "    print(label_names[i],ass)\n",
        "\n",
        "#train_y=pd.get_dummies(y_train)\n",
        "\n",
        "print('df')\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#trains_y=train_y[[0,1]].values\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "print('hi')\n",
        "\n",
        "embed_size=300\n",
        "embedding_matrix=train_embedding_weights\n",
        "max_features=len(train_word_index)+1\n",
        "maxlen=para_max \n",
        "max_sequence_length=para_max\n",
        "MAX_SEQUENCE_LENGTH=para_max\n",
        "EMBEDDING_DIM=300\n",
        "\n",
        "\n",
        "#model3 yoon kim\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable)\n",
        "\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    #x=Dense(256)(embedded_sequences)\n",
        "    x=Dropout(0.5)(embedded_sequences)\n",
        "    x=Bidirectional(LSTM(256))(x)\n",
        "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    #convs = []\n",
        "    #filter_sizes = [3,4,5]\n",
        "\n",
        "    #for filter_size in filter_sizes:\n",
        "        #l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        #l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
        "        #convs.append(l_pool)\n",
        "\n",
        "    #l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
        "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
        "\n",
        "    #if extra_conv==True:\n",
        "        #x = Dropout(0.01)(l_merge)  \n",
        "    #else:\n",
        "        # Original Yoon Kim model\n",
        "        #x = Dropout(0.001)(pool)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    \n",
        "    x = Dropout(0.5)(x)\n",
        "    #x=Dense(256,activation='relu')(x)\n",
        "    # Finally, we feed the output into a Sigmoid layer.\n",
        "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
        "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "    preds = Dense(classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "model1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                 True)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "training_data=yoon_kim_train_data\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "testing_data=yoon_kim_test_data_without_oos\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "#define callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "import time, datetime\n",
        "#start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "history_without_oos = model1.fit(training_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
        "end = datetime.datetime.now()\n",
        "pred_without_oos=model1.predict(yoon_kim_test_data_without_oos)\n",
        "\n",
        "diff1= (end - start)\n",
        "print('time taken by text_6 without_oos',diff1)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def multiclass_metrics_without_oos(cnf_matrix,diff1,lo,val_lo,ac,val_ac):\n",
        "    \n",
        "\t#print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\tclosed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('closed_accuracy',closed_accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(closed_accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"traintime\":diff1,\"loss\":lo,\"val_loss\":val_lo,\"train_acc\":ac,\"val_acc\":val_ac,\"Exceeding_ratio\":excess_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "# In[ ]:\n",
        "import time, datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "original_ans=data_test_without_oos['tag'].tolist()\n",
        "\n",
        "output_class_pred=[]\n",
        "y_test=pred_without_oos\n",
        "y_test=y_test.tolist()\n",
        "for i in range(len(y_test)):\n",
        "    maxx=max(y_test[i])\n",
        "    output_class_pred.append(label_names[y_test[i].index(maxx)])\n",
        "cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "resi_without_oos=multiclass_metrics_without_oos(cnf_matrix,diff1,history_without_oos.history['loss'][-1],history_without_oos.history['val_loss'][-1],history_without_oos.history['acc'][-1],history_without_oos.history['val_acc'][-1])\n",
        "resi_without_oos.to_csv('LSTMresults_text_without_oos.csv', mode='w', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)\n",
        "\n",
        "\n",
        "#as its a fake news classifier , so identifying a fake class will be a TP\n",
        "#resi=check_metric(output_class_pred,original_ans,diff1)\n",
        "\n",
        "#resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
        "\n",
        "################\n",
        "\n",
        "\n",
        "\n",
        "def create_single_line_para(x):\n",
        "    l=[]\n",
        "    for i in x:\n",
        "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
        "    return l\n",
        "        \n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "data_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_without_oos['create_single_line_para']=data_test_without_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_with_oos['create_single_line_para']=data_test_with_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "\n",
        "# In[ ]:\n",
        "print('aas')\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "yoon_kim_train_data=np.array(data_train['create_single_line_para'])\n",
        "yoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')\n",
        "print('dsd')\n",
        "# In[ ]:\n",
        "yoon_kim_test_data=np.array(data_test_without_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_without_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "print('sdsd')\n",
        "\n",
        "yoon_kim_test_data=np.array(data_test_with_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_with_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "\n",
        "print('vsv')\n",
        "\n",
        "#from __future__ import print_function\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
        "from tensorflow.keras.layers import Embedding,concatenate\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "label_names=data_train['tag'].unique()\n",
        "new=pd.get_dummies(data_train['tag'])\n",
        "label_names.tolist()\n",
        "y_train=new[label_names].values\n",
        "\n",
        "\n",
        "# In[133]:\n",
        "\n",
        "\n",
        "print(label_names)\n",
        "classes=len(label_names)\n",
        "\n",
        "print(data_train['tag'])\n",
        "\n",
        "for i in range(classes):\n",
        "    ass=(data_train['tag']==label_names[i]).sum()\n",
        "    print(label_names[i],ass)\n",
        "\n",
        "#train_y=pd.get_dummies(y_train)\n",
        "\n",
        "print('df')\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#trains_y=train_y[[0,1]].values\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "print('hi')\n",
        "\n",
        "embed_size=300\n",
        "embedding_matrix=train_embedding_weights\n",
        "max_features=len(train_word_index)+1\n",
        "maxlen=para_max \n",
        "max_sequence_length=para_max\n",
        "MAX_SEQUENCE_LENGTH=para_max\n",
        "EMBEDDING_DIM=300\n",
        "\n",
        "\n",
        "#model3 yoon kim\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable)\n",
        "\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "    #x=Dense(256)(embedded_sequences)\n",
        "    x=Dropout(0.5)(embedded_sequences)\n",
        "    x=Bidirectional(LSTM(256))(x)\n",
        "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    #convs = []\n",
        "    #filter_sizes = [3,4,5]\n",
        "\n",
        "    #for filter_size in filter_sizes:\n",
        "        #l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        #l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
        "        #convs.append(l_pool)\n",
        "\n",
        "    #l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
        "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
        "\n",
        "    #if extra_conv==True:\n",
        "        #x = Dropout(0.01)(l_merge)  \n",
        "    #else:\n",
        "        # Original Yoon Kim model\n",
        "        #x = Dropout(0.001)(pool)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    \n",
        "    \n",
        "    x = Dropout(0.5)(x)\n",
        "    #x=Dense(256,activation='relu')(x)\n",
        "    # Finally, we feed the output into a Sigmoid layer.\n",
        "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
        "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "    preds = Dense(classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "model1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                 True)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "training_data=yoon_kim_train_data\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "testing_data=yoon_kim_test_data_with_oos\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "#define callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "import time, datetime\n",
        "#start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "history_without_oos = model1.fit(training_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
        "end = datetime.datetime.now()\n",
        "pred_with_oos=model1.predict(yoon_kim_test_data_with_oos)\n",
        "\n",
        "diff1= (end - start)\n",
        "print('time taken by text_6 without_oos',diff1)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "#resi=check_metric(output_class_pred,original_ans,diff1)\n",
        "def check_metric_with_oos_static_thresholding(y_test,original_ans):\n",
        "    output_class_pred=[]\n",
        "    y_test=y_test.tolist()\n",
        "    open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "    pre=[]\n",
        "    recall=[]\n",
        "    f1=[]\n",
        "    specificity=[]\n",
        "    sensitivity=[]\n",
        "    GMean1=[]\n",
        "    Gmean2=[]\n",
        "    MCC=[]\n",
        "    bin_acc=[]\n",
        "    for i in range(len(y_test)):\n",
        "        ma=max(y_test[i])\n",
        "        output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "    #print(y_test)\n",
        "    print(output_class_pred)\n",
        "    \n",
        "    \n",
        "        \n",
        "    for t in [0.5,0.6,0.7,0.8,0.9]: \n",
        "        for i in range(len(y_test)):\n",
        "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                output_class_pred[i][0]=\"oos\"\n",
        "        #print(output_class_pred)\n",
        "        \n",
        "        rightly_predicted_count=0\n",
        "        for i in range(len(y_test)):\n",
        "            if( output_class_pred[i][0]==original_ans[i]):\n",
        "                rightly_predicted_count+=1\n",
        "\n",
        "        #accuracy=rightly_predicted_count/len(y_test)\n",
        "        print('open_accuracy',rightly_predicted_count/len(y_test))\n",
        "        open_accuracy=rightly_predicted_count/len(y_test)\n",
        "        df=[]\n",
        "        TP=0\n",
        "        FP=0\n",
        "        TN=0\n",
        "        FN=0\n",
        "        for i in range(len(y_test)):\n",
        "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                FP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                TP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                FN+=1\n",
        "        \n",
        "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                TN+=1\n",
        "        print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "        binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "    \n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print(\"binary_accuracy\",binary_acc)\n",
        "        print('precision_oos:' ,TP/(TP+FP))\n",
        "        print('recall_oos:',TP/(FN+TP))\n",
        "        print(\"F1_oos:\",F1)\n",
        "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        #open_acc.append(open_accuracy)\n",
        "        bin_acc.append(binary_acc)\n",
        "        open_acc.append(open_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        \n",
        "    matrix=[0.5,0.6,0.7,0.8,0.9]\n",
        "    print(len(matrix))\n",
        "    data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "    metric=pd.DataFrame(data)\n",
        "    return metric,df\n",
        "#resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
        "original_ans=data_test_with_oos['tag'].tolist()\n",
        "resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(pred_with_oos,original_ans)\n",
        "resi_with_oos_static_threshold.to_csv('LSTMresults_text_with_oos_static_threshold.csv', mode='w', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)\n",
        "\n",
        "\n",
        "###################\n",
        "\n",
        "######DOC CNN\n",
        "\n",
        "def create_single_line_para(x):\n",
        "    l=[]\n",
        "    for i in x:\n",
        "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
        "    return l\n",
        "        \n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "data_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_without_oos['create_single_line_para']=data_test_without_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_with_oos['create_single_line_para']=data_test_with_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "\n",
        "# In[ ]:\n",
        "print('aas')\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "yoon_kim_train_data=np.array(data_train['create_single_line_para'])\n",
        "yoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')\n",
        "print('dsd')\n",
        "# In[ ]:\n",
        "yoon_kim_test_data=np.array(data_test_without_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_without_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "print('sdsd')\n",
        "\n",
        "yoon_kim_test_data=np.array(data_test_with_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_with_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "\n",
        "print('vsv')\n",
        "\n",
        "#from __future__ import print_function\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation,Flatten,Bidirectional,GRU,LSTM\n",
        "from tensorflow.keras.layers import Embedding,concatenate\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "label_names=data_train['tag'].unique()\n",
        "new=pd.get_dummies(data_train['tag'])\n",
        "label_names.tolist()\n",
        "y_train=new[label_names].values\n",
        "\n",
        "\n",
        "# In[133]:\n",
        "\n",
        "\n",
        "print(label_names)\n",
        "classes=len(label_names)\n",
        "\n",
        "print(data_train['tag'])\n",
        "\n",
        "for i in range(classes):\n",
        "    ass=(data_train['tag']==label_names[i]).sum()\n",
        "    print(label_names[i],ass)\n",
        "\n",
        "#train_y=pd.get_dummies(y_train)\n",
        "\n",
        "print('df')\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#trains_y=train_y[[0,1]].values\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "print('hi')\n",
        "\n",
        "embed_size=300\n",
        "embedding_matrix=train_embedding_weights\n",
        "max_features=len(train_word_index)+1\n",
        "maxlen=para_max \n",
        "max_sequence_length=para_max\n",
        "MAX_SEQUENCE_LENGTH=para_max\n",
        "EMBEDDING_DIM=300\n",
        "\n",
        "\n",
        "#model3 yoon kim\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable)\n",
        "\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    convs = []\n",
        "    filter_sizes = [3,4,5]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=100, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
        "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
        "\n",
        "    #if extra_conv==True:\n",
        "        #x = Dropout(0.01)(l_merge)  \n",
        "    #else:\n",
        "        # Original Yoon Kim model\n",
        "        #x = Dropout(0.001)(pool)\n",
        "    x = Flatten()(l_merge)\n",
        "    \n",
        "    x = Dropout(0.5)(x)\n",
        "    x=Dense(256,activation='relu')(x)\n",
        "    # Finally, we feed the output into a Sigmoid layer.\n",
        "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
        "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "    preds = Dense(classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "model1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                 True)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "training_data=yoon_kim_train_data\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "testing_data=yoon_kim_test_data_without_oos\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "#define callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "import time, datetime\n",
        "#start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "history_without_oos = model1.fit(training_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
        "end = datetime.datetime.now()\n",
        "pred_without_oos=model1.predict(yoon_kim_test_data_without_oos)\n",
        "\n",
        "diff1= (end - start)\n",
        "print('time taken by text_6 without_oos',diff1)\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "def multiclass_metrics_without_oos(cnf_matrix,diff1,lo,val_lo,ac,val_ac):\n",
        "    \n",
        "\t#print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\tclosed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('closed_accuracy',closed_accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(closed_accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"traintime\":diff1,\"loss\":lo,\"val_loss\":val_lo,\"train_acc\":ac,\"val_acc\":val_ac,\"Exceeding_ratio\":excess_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "# In[ ]:\n",
        "\n",
        "original_ans=data_test_without_oos['tag'].tolist()\n",
        "\n",
        "output_class_pred=[]\n",
        "y_test=pred_without_oos\n",
        "y_test=y_test.tolist()\n",
        "for i in range(len(y_test)):\n",
        "    maxx=max(y_test[i])\n",
        "    output_class_pred.append(label_names[y_test[i].index(maxx)])\n",
        "cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "resi_without_oos=multiclass_metrics_without_oos(cnf_matrix,diff1,history_without_oos.history['loss'][-1],history_without_oos.history['val_loss'][-1],history_without_oos.history['acc'][-1],history_without_oos.history['val_acc'][-1])\n",
        "resi_without_oos.to_csv('CNNresults_text_without_oos.csv', mode='w', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)\n",
        "\n",
        "\n",
        "#as its a fake news classifier , so identifying a fake class will be a TP\n",
        "#resi=check_metric(output_class_pred,original_ans,diff1)\n",
        "\n",
        "#resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
        "\n",
        "##################################\n",
        "\n",
        "\n",
        "def create_single_line_para(x):\n",
        "    l=[]\n",
        "    for i in x:\n",
        "        l+=i    #concatenating all the sentences in a para into a single 1 d arrray\n",
        "    return l\n",
        "        \n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "data_train['create_single_line_para']=data_train['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_without_oos['create_single_line_para']=data_test_without_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "data_test_with_oos['create_single_line_para']=data_test_with_oos['train_seq'].apply(lambda x: create_single_line_para(x) )\n",
        "\n",
        "# In[ ]:\n",
        "print('aas')\n",
        "\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "yoon_kim_train_data=np.array(data_train['create_single_line_para'])\n",
        "yoon_kim_train_data=pad_sequences(yoon_kim_train_data,maxlen=para_max,padding='post')\n",
        "print('dsd')\n",
        "# In[ ]:\n",
        "yoon_kim_test_data=np.array(data_test_without_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_without_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "print('sdsd')\n",
        "\n",
        "yoon_kim_test_data=np.array(data_test_with_oos['create_single_line_para'])\n",
        "yoon_kim_test_data_with_oos=pad_sequences(yoon_kim_test_data,maxlen=para_max,padding='post')\n",
        "\n",
        "print('vsv')\n",
        "\n",
        "#from __future__ import print_function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "label_names=data_train['tag'].unique()\n",
        "new=pd.get_dummies(data_train['tag'])\n",
        "label_names.tolist()\n",
        "y_train=new[label_names].values\n",
        "\n",
        "\n",
        "# In[133]:\n",
        "\n",
        "\n",
        "print(label_names)\n",
        "classes=len(label_names)\n",
        "\n",
        "print(data_train['tag'])\n",
        "\n",
        "for i in range(classes):\n",
        "    ass=(data_train['tag']==label_names[i]).sum()\n",
        "    print(label_names[i],ass)\n",
        "\n",
        "#train_y=pd.get_dummies(y_train)\n",
        "\n",
        "print('df')\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "#trains_y=train_y[[0,1]].values\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "print('hi')\n",
        "\n",
        "embed_size=300\n",
        "embedding_matrix=train_embedding_weights\n",
        "max_features=len(train_word_index)+1\n",
        "maxlen=para_max \n",
        "max_sequence_length=para_max\n",
        "MAX_SEQUENCE_LENGTH=para_max\n",
        "EMBEDDING_DIM=300\n",
        "\n",
        "\n",
        "#model3 yoon kim\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "def ConvNet(embeddings, max_sequence_length, num_words, embedding_dim, trainable=True, extra_conv=False):\n",
        "    \n",
        "    embedding_layer = Embedding(num_words,\n",
        "                            embedding_dim,\n",
        "                            weights=[embeddings],\n",
        "                            input_length=max_sequence_length,\n",
        "                            trainable=trainable)\n",
        "\n",
        "    sequence_input = Input(shape=(max_sequence_length,), dtype='int32')\n",
        "    embedded_sequences = embedding_layer(sequence_input)\n",
        "\n",
        "    # Yoon Kim model (https://arxiv.org/abs/1408.5882)\n",
        "    convs = []\n",
        "    filter_sizes = [3,4,5]\n",
        "\n",
        "    for filter_size in filter_sizes:\n",
        "        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)\n",
        "        l_pool = MaxPooling1D(pool_size=2)(l_conv)\n",
        "        convs.append(l_pool)\n",
        "\n",
        "    l_merge = concatenate(convs, axis=1)\n",
        "\n",
        "    # add a 1D convnet with global maxpooling, instead of Yoon Kim model\n",
        "    #conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)\n",
        "    #pool = MaxPooling1D(pool_size=2)(conv)\n",
        "\n",
        "    #if extra_conv==True:\n",
        "        #x = Dropout(0.01)(l_merge)  \n",
        "    #else:\n",
        "        # Original Yoon Kim model\n",
        "        #x = Dropout(0.001)(pool)\n",
        "    x = Flatten()(l_merge)\n",
        "    \n",
        "    x = Dropout(0.5)(x)\n",
        "    x=Dense(256,activation='relu')(x)\n",
        "    # Finally, we feed the output into a Sigmoid layer.\n",
        "    # The reason why sigmoid is used is because we are trying to achieve a binary classification(1,0) \n",
        "    # for each of the 6 labels, and the sigmoid function will squash the output between the bounds of 0 and 1.\n",
        "    preds = Dense(classes, activation='sigmoid')(x)\n",
        "\n",
        "    model = Model(sequence_input, preds)\n",
        "    model.compile(loss='binary_crossentropy',\n",
        "                  optimizer='Adam',\n",
        "                  metrics=['acc'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "model1 = ConvNet(train_embedding_weights, MAX_SEQUENCE_LENGTH, len(train_word_index)+1, EMBEDDING_DIM, \n",
        "                 True)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "training_data=yoon_kim_train_data\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "testing_data=yoon_kim_test_data_with_oos\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "\n",
        "\n",
        "#define callbacks\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "import time, datetime\n",
        "#start = datetime.datetime.now()\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "history_without_oos = model1.fit(training_data, y_train,  epochs=10,callbacks=callbacks_list,batch_size=32,validation_split=0.1 )\n",
        "end = datetime.datetime.now()\n",
        "pred_with_oos=model1.predict(yoon_kim_test_data_with_oos)\n",
        "\n",
        "diff1= (end - start)\n",
        "print('time taken by text_6 without_oos',diff1)\n",
        "\n",
        "\n",
        "# In[ ]:\n",
        "\n",
        "#resi=check_metric(output_class_pred,original_ans,diff1)\n",
        "\n",
        "#resi.to_csv('results_text', mode='a', index = False, header=resi.columns,columns=resi.columns)\n",
        "\n",
        "############################\n",
        "def check_metric_with_oos_static_thresholding(y_test,original_ans):\n",
        "    output_class_pred=[]\n",
        "    y_test=y_test.tolist()\n",
        "    open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "    pre=[]\n",
        "    recall=[]\n",
        "    f1=[]\n",
        "    specificity=[]\n",
        "    sensitivity=[]\n",
        "    GMean1=[]\n",
        "    Gmean2=[]\n",
        "    MCC=[]\n",
        "    bin_acc=[]\n",
        "    for i in range(len(y_test)):\n",
        "        ma=max(y_test[i])\n",
        "        output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "    #print(y_test)\n",
        "    print(output_class_pred)\n",
        "    \n",
        "    \n",
        "        \n",
        "    for t in [0.5,0.6,0.7,0.8,0.9]: \n",
        "        for i in range(len(y_test)):\n",
        "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                output_class_pred[i][0]=\"oos\"\n",
        "        #print(output_class_pred)\n",
        "        \n",
        "        rightly_predicted_count=0\n",
        "        for i in range(len(y_test)):\n",
        "            if( output_class_pred[i][0]==original_ans[i]):\n",
        "                rightly_predicted_count+=1\n",
        "\n",
        "        #accuracy=rightly_predicted_count/len(y_test)\n",
        "        print('open_accuracy',rightly_predicted_count/len(y_test))\n",
        "        open_accuracy=rightly_predicted_count/len(y_test)\n",
        "        df=[]\n",
        "        TP=0\n",
        "        FP=0\n",
        "        TN=0\n",
        "        FN=0\n",
        "        for i in range(len(y_test)):\n",
        "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                FP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                TP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                FN+=1\n",
        "        \n",
        "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                TN+=1\n",
        "        print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "        binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "    \n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print(\"binary_accuracy\",binary_acc)\n",
        "        print('precision_oos:' ,TP/(TP+FP))\n",
        "        print('recall_oos:',TP/(FN+TP))\n",
        "        print(\"F1_oos:\",F1)\n",
        "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        #open_acc.append(open_accuracy)\n",
        "        bin_acc.append(binary_acc)\n",
        "        open_acc.append(open_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        \n",
        "    matrix=[0.5,0.6,0.7,0.8,0.9]\n",
        "    print(len(matrix))\n",
        "    data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "    metric=pd.DataFrame(data)\n",
        "    return metric,df\n",
        "original_ans=data_test_with_oos['tag'].tolist()\n",
        "resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(pred_with_oos,original_ans)\n",
        "resi_with_oos_static_threshold.to_csv('CNNresults_text_with_oos_static_threshold.csv', mode='w', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-17T18:59:20.883856Z",
          "iopub.execute_input": "2021-07-17T18:59:20.884324Z",
          "iopub.status.idle": "2021-07-17T19:01:06.23384Z",
          "shell.execute_reply.started": "2021-07-17T18:59:20.884282Z",
          "shell.execute_reply": "2021-07-17T19:01:06.232984Z"
        },
        "trusted": true,
        "id": "WjI16ttD2lha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRYING OUR MODEL"
      ],
      "metadata": {
        "id": "iJwtov1k2lhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the latest Tensorflow version.\n",
        "!pip3 install --quiet \"tensorflow>=1.7\"\n",
        "# Install TF-Hub.\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet seaborn\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-09T15:33:41.308954Z",
          "iopub.execute_input": "2022-01-09T15:33:41.309335Z",
          "iopub.status.idle": "2022-01-09T15:34:03.877934Z",
          "shell.execute_reply.started": "2022-01-09T15:33:41.3093Z",
          "shell.execute_reply": "2022-01-09T15:34:03.876581Z"
        },
        "trusted": true,
        "id": "hg7uUI_f2lhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:49:48.302071Z",
          "iopub.execute_input": "2022-01-12T13:49:48.302462Z",
          "iopub.status.idle": "2022-01-12T13:49:48.444473Z",
          "shell.execute_reply.started": "2022-01-12T13:49:48.302428Z",
          "shell.execute_reply": "2022-01-12T13:49:48.443563Z"
        },
        "trusted": true,
        "id": "5Rarb0Eo2lhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#embedd=['bertbase','distilbert','glove','roberta','universal']\n",
        "K=[15]\n",
        "embedd='universal'\n",
        "for k_nearest in K:\n",
        "    embedding=embedd\n",
        "    def make_embeddings(data,name):\n",
        "        if(name=='normal-message'):\n",
        "            print('sd')\n",
        "            model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "            print('ee')\n",
        "            #---------------\n",
        "        if name=='bertbase':\n",
        "            model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "            #---------------------------\n",
        "        if (name=='distilbert'):\n",
        "            model = SentenceTransformer('stsb-distilbert-base')\n",
        "            #-------------\n",
        "        if name=='glove':\n",
        "            model = SentenceTransformer('average_word_embeddings_glove.6B.300d')\n",
        "        #--------------------------------------\n",
        "        if (name=='roberta'):\n",
        "            model = SentenceTransformer('stsb-roberta-base')\n",
        "\n",
        "        #------------------------------------\n",
        "\n",
        "\n",
        "        if name=='universal':\n",
        "            module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/5'\n",
        "            embed = hub.KerasLayer(module_url, trainable=True, name='USE_embedding')\n",
        "            return embed(data['title']).numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        X_reduced= model.encode(data['title'])\n",
        "        return X_reduced\n",
        "    X_tr=make_embeddings(data_train,embedding)\n",
        "    X_te=make_embeddings(data_test_without_oos,embedding)\n",
        "\n",
        "\n",
        "    def neighborhood_search(emb,thresh):\n",
        "\n",
        "        index = faiss.IndexFlatIP(emb.shape[1])\n",
        "        faiss.normalize_L2(emb)\n",
        "        index.add(emb)\n",
        "        faiss.write_index(index,'yodd')\n",
        "\n",
        "        sim, I = index.search(emb, k_nearest)\n",
        "        #print(emb)\n",
        "        #print(I[32])\n",
        "        print(sim)\n",
        "\n",
        "        '''\n",
        "        pred_index=[]\n",
        "        pred_sim=[]\n",
        "        for i in range((emb.shape[0])):\n",
        "            cut_index=0\n",
        "            for j in sim[i]:\n",
        "                if(j>thresh):\n",
        "                    cut_index+=1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            #cut_index =  np.searchsorted(sim[i],thresh)\n",
        "            pred_index .append( I[i][:(cut_index)])\n",
        "            pred_sim .append (sim[i][:(cut_index)])\n",
        "        #print(pred_index[32])\n",
        "        #print(pred_sim[32])\n",
        "\n",
        "        return pred_index,pred_sim\n",
        "        '''\n",
        "        return I,sim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def blend_neighborhood(emb, match_index_lst, similarities_lst):\n",
        "        new_emb = emb.copy()\n",
        "        for i in range(emb.shape[0]):\n",
        "            cur_emb = emb[match_index_lst[i]]\n",
        "            #print(cur_emb)\n",
        "            weights = np.expand_dims(similarities_lst[i], 1)\n",
        "            #print(weights)\n",
        "            new_emb[i] = (cur_emb * weights).sum(axis=0)\n",
        "            #print(new_emb[i])\n",
        "        new_emb = normalize(new_emb, axis=1)\n",
        "        #print(weights)\n",
        "        #print(new_emb[199])\n",
        "        #print(new_emb)\n",
        "        return new_emb\n",
        "\n",
        "\n",
        "    threshes=[0.5]#must be a list of thresholds kyunki iteratively similarities badhti jaani  chahiye\n",
        "    def iterative_neighborhood_blending(emb, threshes):\n",
        "        for thresh in threshes:\n",
        "            match_index_lst, similarities_lst = neighborhood_search(emb, thresh)\n",
        "            emb = blend_neighborhood(emb, match_index_lst, similarities_lst)\n",
        "        return match_index_lst,similarities_lst,emb\n",
        "    #match_index_lst,similarities_lst,X_tr=iterative_neighborhood_blending(X_tr, threshes)\n",
        "    match_index_lst,similarities_lst=neighborhood_search(X_tr,0)\n",
        "    X_tr= blend_neighborhood(X_tr, match_index_lst, similarities_lst)\n",
        "\n",
        "    simi=cosine_similarity(X_te,X_tr)\n",
        "    output_class_pred=[]\n",
        "    for i in range(len(simi)):\n",
        "        ma=np.argmax(simi[i])\n",
        "        output_class_pred.append(data_train['tag'][ma])\n",
        "\n",
        "    original_ans=data_test_without_oos['tag'].tolist()\n",
        "    \n",
        "    def multiclass_metrics_without_oos(cnf_matrix):\n",
        "\n",
        "        #print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "        cnf_matrix=np.asarray(cnf_matrix)\n",
        "        FP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "        FN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "        TP = np.diag(cnf_matrix)\n",
        "        TN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "        FP = FP.astype(float)\n",
        "        FN = FN.astype(float)\n",
        "        TP = TP.astype(float)\n",
        "        TN = TN.astype(float)\n",
        "\n",
        "        TP=np.sum(TP)\n",
        "        TN=np.sum(TN)\n",
        "        FP=np.sum(FP)\n",
        "        FN=np.sum(FN)\n",
        "\n",
        "\n",
        "        closed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "\n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print('closed_accuracy',closed_accuracy)\n",
        "        print('precision:' ,TP/(TP+FP))\n",
        "        print('recall:',TP/(FN+TP))\n",
        "        print(\"F1:\",F1)\n",
        "        print(\"Specificity:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        acc=[]\n",
        "        pre=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        specificity=[]\n",
        "        sensitivity=[]\n",
        "        GMean1=[]\n",
        "        Gmean2=[]\n",
        "        MCC=[]\n",
        "        tp=[]\n",
        "        fp=[]\n",
        "        fn=[]\n",
        "        tn=[]\n",
        "        acc.append(closed_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        tp.append(TP)\n",
        "        fp.append(FP)\n",
        "        tn.append(TN)\n",
        "        fn.append(FN)\n",
        "        data={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
        "        metric=pd.DataFrame(data)\n",
        "        return metric\n",
        "    cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "    resi_without_oos=multiclass_metrics_without_oos(cnf_matrix)\n",
        "    resi_without_oos.to_csv('newmithodresults_text_without_oos.csv', mode='a', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)\n",
        "\n",
        "\n",
        "    '''def make_embeddings(data,name):\n",
        "        if(name=='normal-message'):\n",
        "            print('sd')\n",
        "            model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "            print('ee')\n",
        "        if(name=='roberta'):\n",
        "            model = SentenceTransformer('stsb-roberta-base')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "        return X_reduced\n",
        "    '''\n",
        "    X_tr=make_embeddings(data_train,embedding)\n",
        "    X_te=make_embeddings(data_test_with_oos,embedding)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def neighborhood_search(emb,thresh):\n",
        "\n",
        "        index = faiss.IndexFlatIP(emb.shape[1])\n",
        "        faiss.normalize_L2(emb)\n",
        "        index.add(emb)\n",
        "        faiss.write_index(index,'yodd')\n",
        "\n",
        "        sim, I = index.search(emb, k_nearest)\n",
        "        #print(emb)\n",
        "        #print(I[32])\n",
        "        print(sim)\n",
        "\n",
        "        '''\n",
        "        pred_index=[]\n",
        "        pred_sim=[]\n",
        "        for i in range((emb.shape[0])):\n",
        "            cut_index=0\n",
        "            for j in sim[i]:\n",
        "                if(j>thresh):\n",
        "                    cut_index+=1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            #cut_index =  np.searchsorted(sim[i],thresh)\n",
        "            pred_index .append( I[i][:(cut_index)])\n",
        "            pred_sim .append (sim[i][:(cut_index)])\n",
        "        #print(pred_index[32])\n",
        "        #print(pred_sim[32])\n",
        "\n",
        "        return pred_index,pred_sim\n",
        "        '''\n",
        "        return I,sim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def blend_neighborhood(emb, match_index_lst, similarities_lst):\n",
        "        new_emb = emb.copy()\n",
        "        for i in range(emb.shape[0]):\n",
        "            cur_emb = emb[match_index_lst[i]]\n",
        "            #print(cur_emb)\n",
        "            weights = np.expand_dims(similarities_lst[i], 1)\n",
        "            #print(weights)\n",
        "            new_emb[i] = (cur_emb * weights).sum(axis=0)\n",
        "            #print(new_emb[i])\n",
        "        new_emb = normalize(new_emb, axis=1)\n",
        "        #print(weights)\n",
        "        #print(new_emb[199])\n",
        "        #print(new_emb)\n",
        "        return new_emb\n",
        "\n",
        "\n",
        "    threshes=[0.5]#must be a list of thresholds kyunki iteratively similarities badhti jaani  chahiye\n",
        "    def iterative_neighborhood_blending(emb, threshes):\n",
        "        for thresh in threshes:\n",
        "            match_index_lst, similarities_lst = neighborhood_search(emb, thresh)\n",
        "            emb = blend_neighborhood(emb, match_index_lst, similarities_lst)\n",
        "        return match_index_lst,similarities_lst,emb\n",
        "    #match_index_lst,similarities_lst,X_tr=iterative_neighborhood_blending(X_tr, threshes)\n",
        "    match_index_lst,similarities_lst=neighborhood_search(X_tr,0)\n",
        "    X_tr= blend_neighborhood(X_tr, match_index_lst, similarities_lst)\n",
        "\n",
        "\n",
        "\n",
        "    simi=cosine_similarity(X_te,X_tr)\n",
        "    pred_with_oos=[]\n",
        "\n",
        "    #for i in range(len(simi)):\n",
        "        #ma=np.max(simi[i])\n",
        "        #ind=np.argmax(simi[i])\n",
        "        #if(ma>0.5):\n",
        "            #output_class_pred.append(data_train['tag'][ind])\n",
        "        #else:\n",
        "            #output_class_pred.append('oos')\n",
        "\n",
        "\n",
        "\n",
        "    #original_ans=data_test_without_oos['tag'].tolist()\n",
        "   \n",
        "    def check_metric_with_oos_static_thresholding(original_ans):\n",
        "        output_class_pred=[]\n",
        "        #y_test=y_test\n",
        "        open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "        pre=[]\n",
        "        recall=[]\n",
        "        f1=[]\n",
        "        specificity=[]\n",
        "        sensitivity=[]\n",
        "        GMean1=[]\n",
        "        Gmean2=[]\n",
        "        MCC=[]\n",
        "        bin_acc=[]\n",
        "        '''for i in range(len(y_test)):\n",
        "            ma=max(y_test[i])\n",
        "            output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "        #print(y_test)\n",
        "        print(output_class_pred)\n",
        "        '''\n",
        "        simi=cosine_similarity(X_te,X_tr)\n",
        "        pred_with_oos=[]\n",
        "\n",
        "        for i in range(len(simi)):\n",
        "            ma=np.max(simi[i])\n",
        "            ind=np.argmax(simi[i])\n",
        "            output_class_pred.append([data_train['tag'][ind],ma])\n",
        "\n",
        "        print()\n",
        "        y_test=output_class_pred\n",
        "\n",
        "\n",
        "        for t in [0.5,0.6,0.7,0.8,0.9]: \n",
        "            for i in range(len(output_class_pred)):\n",
        "                if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                    output_class_pred[i][0]=\"oos\"\n",
        "            #print(output_class_pred)\n",
        "\n",
        "            rightly_predicted_count=0\n",
        "            for i in range(len(output_class_pred)):\n",
        "                if( output_class_pred[i][0]==original_ans[i]):\n",
        "                    rightly_predicted_count+=1\n",
        "\n",
        "            #accuracy=rightly_predicted_count/len(y_test)\n",
        "            print('open_accuracy',rightly_predicted_count/len(output_class_pred))\n",
        "            open_accuracy=rightly_predicted_count/len(output_class_pred)\n",
        "            df=[]\n",
        "            TP=0\n",
        "            FP=0\n",
        "            TN=0\n",
        "            FN=0\n",
        "            for i in range(len(y_test)):\n",
        "                if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                    FP+=1\n",
        "                    df.append(data_test_with_oos.loc[i,'title'])\n",
        "\n",
        "                if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                    TP+=1\n",
        "                    df.append(data_test_with_oos.loc[i,'title'])\n",
        "\n",
        "                if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                    FN+=1\n",
        "\n",
        "                if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                    TN+=1\n",
        "            print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "            binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "            precision=TP/(TP+FP)\n",
        "            recalll=TP/(FN+TP)\n",
        "            F1=2*precision*recalll/(precision+recalll)\n",
        "            sensiti=TP/(TP+FN)\n",
        "            specifici=TN/(TN+FP)\n",
        "            numerator=TP*TN - FP*FN\n",
        "\n",
        "            denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "            MCc=numerator/denominator\n",
        "            G_mean1=np.sqrt(sensiti*precision)\n",
        "            G_mean2=np.sqrt(sensiti*specifici)\n",
        "            print(\"binary_accuracy\",binary_acc)\n",
        "            print('precision_oos:' ,TP/(TP+FP))\n",
        "            print('recall_oos:',TP/(FN+TP))\n",
        "            print(\"F1_oos:\",F1)\n",
        "            print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "            print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "            print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "            print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "            print(\"MCC :\",MCc)\n",
        "            #open_acc.append(open_accuracy)\n",
        "            bin_acc.append(binary_acc)\n",
        "            open_acc.append(open_accuracy)\n",
        "            pre.append(precision)\n",
        "            recall.append(recalll)\n",
        "            f1.append(F1)\n",
        "            specificity.append(specifici)\n",
        "            sensitivity.append(sensiti)\n",
        "            GMean1.append(G_mean1)\n",
        "            Gmean2.append(G_mean2)\n",
        "            MCC.append(MCc)\n",
        "\n",
        "        matrix=[0.5,0.6,0.7,0.8,0.9]\n",
        "        print(len(matrix))\n",
        "        data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "        metric=pd.DataFrame(data)\n",
        "        return metric,df\n",
        "    original_ans=data_test_with_oos['tag'].tolist()\n",
        "    resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(original_ans)\n",
        "    resi_with_oos_static_threshold.to_csv('new_mrthod_oos_static_threshold.csv', mode='a', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:49:49.663307Z",
          "iopub.execute_input": "2022-01-12T13:49:49.66365Z",
          "iopub.status.idle": "2022-01-12T13:51:39.019884Z",
          "shell.execute_reply.started": "2022-01-12T13:49:49.663618Z",
          "shell.execute_reply": "2022-01-12T13:51:39.018742Z"
        },
        "trusted": true,
        "id": "Ql42P1Na2lho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_te=make_embeddings(data_test_without_oos,'normal-message')\n",
        "\n",
        "\n",
        "def neighborhood_search(emb,thresh):\n",
        "    \n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    faiss.normalize_L2(emb)\n",
        "    index.add(emb)\n",
        "    faiss.write_index(index,'yodd')\n",
        "    \n",
        "    sim, I = index.search(emb, 15)\n",
        "    #print(emb)\n",
        "    #print(I[32])\n",
        "    print(sim)\n",
        "    \n",
        "    '''\n",
        "    pred_index=[]\n",
        "    pred_sim=[]\n",
        "    for i in range((emb.shape[0])):\n",
        "        cut_index=0\n",
        "        for j in sim[i]:\n",
        "            if(j>thresh):\n",
        "                cut_index+=1\n",
        "            else:\n",
        "                break\n",
        "                \n",
        "        #cut_index =  np.searchsorted(sim[i],thresh)\n",
        "        pred_index .append( I[i][:(cut_index)])\n",
        "        pred_sim .append (sim[i][:(cut_index)])\n",
        "    #print(pred_index[32])\n",
        "    #print(pred_sim[32])\n",
        "    \n",
        "    return pred_index,pred_sim\n",
        "    '''\n",
        "    return I,sim\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "def blend_neighborhood(emb, match_index_lst, similarities_lst):\n",
        "    new_emb = emb.copy()\n",
        "    for i in range(emb.shape[0]):\n",
        "        cur_emb = emb[match_index_lst[i]]\n",
        "        #print(cur_emb)\n",
        "        weights = np.expand_dims(similarities_lst[i], 1)\n",
        "        #print(weights)\n",
        "        new_emb[i] = (cur_emb * weights).sum(axis=0)\n",
        "        #print(new_emb[i])\n",
        "    new_emb = normalize(new_emb, axis=1)\n",
        "    #print(weights)\n",
        "    #print(new_emb[199])\n",
        "    #print(new_emb)\n",
        "    return new_emb\n",
        "\n",
        "\n",
        "threshes=[0.5]#must be a list of thresholds kyunki iteratively similarities badhti jaani  chahiye\n",
        "def iterative_neighborhood_blending(emb, threshes):\n",
        "    for thresh in threshes:\n",
        "        match_index_lst, similarities_lst = neighborhood_search(emb, thresh)\n",
        "        emb = blend_neighborhood(emb, match_index_lst, similarities_lst)\n",
        "    return match_index_lst,similarities_lst,emb\n",
        "#match_index_lst,similarities_lst,X_tr=iterative_neighborhood_blending(X_tr, threshes)\n",
        "match_index_lst,similarities_lst=neighborhood_search(X_tr,0)\n",
        "X_tr= blend_neighborhood(X_tr, match_index_lst, similarities_lst)\n",
        "\n",
        "simi=cosine_similarity(X_te,X_tr)\n",
        "output_class_pred=[]\n",
        "for i in range(len(simi)):\n",
        "    ma=np.argmax(simi[i])\n",
        "    output_class_pred.append(data_train['tag'][ma])\n",
        "    \n",
        "original_ans=data_test_without_oos['tag'].tolist()\n",
        "\n",
        "def multiclass_metrics_without_oos(cnf_matrix):\n",
        "    \n",
        "\t#print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\tclosed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('closed_accuracy',closed_accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(closed_accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"Exceeding_ratio\":excess_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "\n",
        "cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "resi_without_oos=multiclass_metrics_without_oos(cnf_matrix)\n",
        "resi_without_oos.to_csv('newmithodresults_text_without_oos.csv', mode='w', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)\n",
        "\n",
        "\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_te=make_embeddings(data_test_with_oos,'normal-message')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def neighborhood_search(emb,thresh):\n",
        "    \n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    faiss.normalize_L2(emb)\n",
        "    index.add(emb)\n",
        "    faiss.write_index(index,'yodd')\n",
        "    \n",
        "    sim, I = index.search(emb, 15)\n",
        "    #print(emb)\n",
        "    #print(I[32])\n",
        "    print(sim)\n",
        "    \n",
        "    '''\n",
        "    pred_index=[]\n",
        "    pred_sim=[]\n",
        "    for i in range((emb.shape[0])):\n",
        "        cut_index=0\n",
        "        for j in sim[i]:\n",
        "            if(j>thresh):\n",
        "                cut_index+=1\n",
        "            else:\n",
        "                break\n",
        "                \n",
        "        #cut_index =  np.searchsorted(sim[i],thresh)\n",
        "        pred_index .append( I[i][:(cut_index)])\n",
        "        pred_sim .append (sim[i][:(cut_index)])\n",
        "    #print(pred_index[32])\n",
        "    #print(pred_sim[32])\n",
        "    \n",
        "    return pred_index,pred_sim\n",
        "    '''\n",
        "    return I,sim\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "def blend_neighborhood(emb, match_index_lst, similarities_lst):\n",
        "    new_emb = emb.copy()\n",
        "    for i in range(emb.shape[0]):\n",
        "        cur_emb = emb[match_index_lst[i]]\n",
        "        #print(cur_emb)\n",
        "        weights = np.expand_dims(similarities_lst[i], 1)\n",
        "        #print(weights)\n",
        "        new_emb[i] = (cur_emb * weights).sum(axis=0)\n",
        "        #print(new_emb[i])\n",
        "    new_emb = normalize(new_emb, axis=1)\n",
        "    #print(weights)\n",
        "    #print(new_emb[199])\n",
        "    #print(new_emb)\n",
        "    return new_emb\n",
        "\n",
        "\n",
        "threshes=[0.5]#must be a list of thresholds kyunki iteratively similarities badhti jaani  chahiye\n",
        "def iterative_neighborhood_blending(emb, threshes):\n",
        "    for thresh in threshes:\n",
        "        match_index_lst, similarities_lst = neighborhood_search(emb, thresh)\n",
        "        emb = blend_neighborhood(emb, match_index_lst, similarities_lst)\n",
        "    return match_index_lst,similarities_lst,emb\n",
        "#match_index_lst,similarities_lst,X_tr=iterative_neighborhood_blending(X_tr, threshes)\n",
        "match_index_lst,similarities_lst=neighborhood_search(X_tr,0)\n",
        "X_tr= blend_neighborhood(X_tr, match_index_lst, similarities_lst)\n",
        "\n",
        "\n",
        "\n",
        "simi=cosine_similarity(X_te,X_tr)\n",
        "pred_with_oos=[]\n",
        "\n",
        "#for i in range(len(simi)):\n",
        "    #ma=np.max(simi[i])\n",
        "    #ind=np.argmax(simi[i])\n",
        "    #if(ma>0.5):\n",
        "        #output_class_pred.append(data_train['tag'][ind])\n",
        "    #else:\n",
        "        #output_class_pred.append('oos')\n",
        "\n",
        "        \n",
        "    \n",
        "#original_ans=data_test_without_oos['tag'].tolist()\n",
        "\n",
        "def check_metric_with_oos_static_thresholding(original_ans):\n",
        "    output_class_pred=[]\n",
        "    #y_test=y_test\n",
        "    open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "    pre=[]\n",
        "    recall=[]\n",
        "    f1=[]\n",
        "    specificity=[]\n",
        "    sensitivity=[]\n",
        "    GMean1=[]\n",
        "    Gmean2=[]\n",
        "    MCC=[]\n",
        "    bin_acc=[]\n",
        "    '''for i in range(len(y_test)):\n",
        "        ma=max(y_test[i])\n",
        "        output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "    #print(y_test)\n",
        "    print(output_class_pred)\n",
        "    '''\n",
        "    simi=cosine_similarity(X_te,X_tr)\n",
        "    pred_with_oos=[]\n",
        "\n",
        "    for i in range(len(simi)):\n",
        "        ma=np.max(simi[i])\n",
        "        ind=np.argmax(simi[i])\n",
        "        output_class_pred.append([data_train['tag'][ind],ma])\n",
        "        \n",
        "    print()\n",
        "    y_test=output_class_pred\n",
        "    \n",
        "        \n",
        "    for t in [0.3,0.4,0.5,0.6,0.7,0.8,0.9]: \n",
        "        for i in range(len(output_class_pred)):\n",
        "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                output_class_pred[i][0]=\"oos\"\n",
        "        #print(output_class_pred)\n",
        "        \n",
        "        rightly_predicted_count=0\n",
        "        for i in range(len(output_class_pred)):\n",
        "            if( output_class_pred[i][0]==original_ans[i]):\n",
        "                rightly_predicted_count+=1\n",
        "\n",
        "        #accuracy=rightly_predicted_count/len(y_test)\n",
        "        print('open_accuracy',rightly_predicted_count/len(output_class_pred))\n",
        "        open_accuracy=rightly_predicted_count/len(output_class_pred)\n",
        "        df=[]\n",
        "        TP=0\n",
        "        FP=0\n",
        "        TN=0\n",
        "        FN=0\n",
        "        for i in range(len(y_test)):\n",
        "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                FP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                TP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                FN+=1\n",
        "        \n",
        "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                TN+=1\n",
        "        print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "        binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "    \n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print(\"binary_accuracy\",binary_acc)\n",
        "        print('precision_oos:' ,TP/(TP+FP))\n",
        "        print('recall_oos:',TP/(FN+TP))\n",
        "        print(\"F1_oos:\",F1)\n",
        "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        #open_acc.append(open_accuracy)\n",
        "        bin_acc.append(binary_acc)\n",
        "        open_acc.append(open_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        \n",
        "    matrix=[0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
        "    print(len(matrix))\n",
        "    data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "    metric=pd.DataFrame(data)\n",
        "    return metric,df\n",
        "\n",
        "\n",
        "\n",
        "simi=cosine_similarity(X_te,X_tr)\n",
        "pred_with_oos=[]\n",
        "original_ans=data_test_with_oos['tag'].tolist()\n",
        "resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(original_ans)\n",
        "resi_with_oos_static_threshold.to_csv('new_mrthod_oos_static_threshold.csv', mode='w', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T17:57:22.883237Z",
          "iopub.execute_input": "2021-07-21T17:57:22.883627Z",
          "iopub.status.idle": "2021-07-21T17:58:16.306343Z",
          "shell.execute_reply.started": "2021-07-21T17:57:22.88359Z",
          "shell.execute_reply": "2021-07-21T17:58:16.304052Z"
        },
        "trusted": true,
        "id": "Tw0BxReY2lht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T18:04:19.63561Z",
          "iopub.execute_input": "2021-07-21T18:04:19.636019Z",
          "iopub.status.idle": "2021-07-21T18:04:31.594968Z",
          "shell.execute_reply.started": "2021-07-21T18:04:19.635984Z",
          "shell.execute_reply": "2021-07-21T18:04:31.594055Z"
        },
        "trusted": true,
        "id": "N_o4ngTq2lhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['title'])\n",
        "    return X_reduced\n",
        "X_tr=make_embeddings(data_train,'normal-message')\n",
        "X_train=X_tr\n",
        "#X_te=make_embeddings(data_test_without_oos,'normal-message')\n",
        "\n",
        "\n",
        "def neighborhood_search(emb,thresh):\n",
        "    \n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    faiss.normalize_L2(emb)\n",
        "    index.add(emb)\n",
        "    faiss.write_index(index,'yodd')\n",
        "    \n",
        "    sim, I = index.search(emb, 15)\n",
        "    #print(emb)\n",
        "    #print(I[32])\n",
        "    print(sim)\n",
        "    \n",
        "    '''\n",
        "    pred_index=[]\n",
        "    pred_sim=[]\n",
        "    for i in range((emb.shape[0])):\n",
        "        cut_index=0\n",
        "        for j in sim[i]:\n",
        "            if(j>thresh):\n",
        "                cut_index+=1\n",
        "            else:\n",
        "                break\n",
        "                \n",
        "        #cut_index =  np.searchsorted(sim[i],thresh)\n",
        "        pred_index .append( I[i][:(cut_index)])\n",
        "        pred_sim .append (sim[i][:(cut_index)])\n",
        "    #print(pred_index[32])\n",
        "    #print(pred_sim[32])\n",
        "    \n",
        "    return pred_index,pred_sim\n",
        "    '''\n",
        "    return I,sim\n",
        "\n",
        "match_index_lst,similarities_lst=neighborhood_search(X_tr,0)\n",
        "#X_tr= blend_neighborhood(X_tr, match_index_lst, similarities_lst)\n",
        "#match_index_lst,similarities_lst=neighborhood_search(X_tr,0)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T18:22:44.73245Z",
          "iopub.execute_input": "2021-07-21T18:22:44.732807Z",
          "iopub.status.idle": "2021-07-21T18:24:56.037019Z",
          "shell.execute_reply.started": "2021-07-21T18:22:44.732772Z",
          "shell.execute_reply": "2021-07-21T18:24:56.036089Z"
        },
        "trusted": true,
        "id": "0Qg7kZMY2lhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_tr[5])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-07-21T18:21:47.321147Z",
          "iopub.execute_input": "2021-07-21T18:21:47.321488Z",
          "iopub.status.idle": "2021-07-21T18:21:47.328946Z",
          "shell.execute_reply.started": "2021-07-21T18:21:47.321457Z",
          "shell.execute_reply": "2021-07-21T18:21:47.328008Z"
        },
        "trusted": true,
        "id": "c5mzYryu2lhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2EJ3iWrm2lhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEXTCNN"
      ],
      "metadata": {
        "id": "TmVgz_Xc2lhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "filter_sizes = [1,2,3,4]\n",
        "num_filters = 64\n",
        "embed_size=300\n",
        "embedding_matrix=train_embedding_weights\n",
        "max_features=len(train_word_index)+1\n",
        "maxlen=m*n\n",
        "def get_model(loss,activation):    \n",
        "    inp = Input(shape=(maxlen, ),)\n",
        "    emb = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
        "    #x = SpatialDropout1D(0.4)(x)\n",
        "    r=Bidirectional(LSTM(32))(emb)\n",
        "    #j=Bidirectional(RNN(32))(emb)\n",
        "    \n",
        "    x = Reshape((m, n, 300))(emb)\n",
        "    #print(x)\n",
        "    #x=LSTM(32,return_sequences=False)(x)\n",
        "    \n",
        "    conv_0 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 2), \n",
        "                                                                                    activation='relu')(x)\n",
        "    conv_1 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 3),\n",
        "                                                                                    activation='relu')(x)\n",
        "    \n",
        "    #conv_2 = Conv2D(num_filters, kernel_size=(filter_sizes[0], 4),\n",
        "                                                                                    #activation='relu')(x)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    conv_4 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 1), \n",
        "                                                                                    activation='relu')(x)\n",
        "    conv_5 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 2), activation='relu')(x)\n",
        "    \n",
        "    #conv_6 = Conv2D(num_filters, kernel_size=(filter_sizes[1], 3),\n",
        "                                                                                    #activation='relu')(x)\n",
        "    \n",
        "    \n",
        "    \n",
        "    maxpool_0 = MaxPool2D()(conv_0)\n",
        "\n",
        "    maxpool_0=Flatten()(maxpool_0)\n",
        "    maxpool_1 = MaxPool2D()(conv_1)\n",
        "    maxpool_1=Flatten()(maxpool_1)\n",
        "    #maxpool_2 = MaxPool2D()(conv_2)\n",
        "    #maxpool_2 = Flatten()(maxpool_2)\n",
        "    \n",
        "    maxpool_4 = MaxPool2D()(conv_4)\n",
        "    maxpool_4=Flatten()(maxpool_4)\n",
        "    maxpool_5 = MaxPool2D()(conv_5)\n",
        "    maxpool_5=Flatten()(maxpool_5)\n",
        "    #maxpool_6 = MaxPool2D()(conv_6)\n",
        "    #maxpool_6=Flatten()(maxpool_6)\n",
        "    #maxpool_7 = MaxPool2D()(conv_7)\n",
        "   # maxpool_7=Flatten()(maxpool_7)\n",
        "        \n",
        "    w=concatenate([maxpool_4, maxpool_5],axis=1)    \n",
        "    z = concatenate([maxpool_0, maxpool_1],axis=1)\n",
        "    #z = concatenate([maxpool_0, maxpool_1,maxpool_4, maxpool_5],axis=1)\n",
        "    z=concatenate([w,z,r],axis=1)\n",
        "    \n",
        "    \n",
        "    \n",
        "    z=Dense(units=64,activation=\"relu\")(z)\n",
        "    \n",
        "    \n",
        "   # z = Dropout(0.2)(z)\n",
        "        \n",
        "    outp = Dense(units=classes, activation=activation)(z)\n",
        "    \n",
        "    model = Model(inputs=inp, outputs=outp)\n",
        "    model.compile(loss=loss, optimizer='Adam',metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "trusted": true,
        "id": "3YGW4m4K2lhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_without_oos=get_model('categorical_crossentropy','softmax')\n",
        "print(model_without_oos.summary())\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=4, verbose=1)\n",
        "callbacks_list = [early_stopping]\n",
        "\n",
        "start = datetime.datetime.now()\n",
        "history_without_oos = model_without_oos.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,validation_split=0.1 )\n",
        "end = datetime.datetime.now()\n",
        "pred_without_oos=model_without_oos.predict(test_cnn_data_without_oos)\n",
        "\n",
        "diff1= (end - start)\n",
        "print('time taken by text_6 without_oos',diff1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "R66ugHbK2lhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "f3-UD-ut2lhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab=len(TRAINING_VOCAB)\n",
        "\n",
        "def multiclass_metrics_without_oos(cnf_matrix,diff1,lo,val_lo,ac,val_ac):\n",
        "    \n",
        "\t#print(roc_auc_score(original_ans, model_without_oos.predict_proba(test_cnn_data_without_oos)[:, 1]))\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\tclosed_accuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('closed_accuracy',closed_accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(closed_accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'closed_accuracy':acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn,\"traintime\":diff1,\"loss\":lo,\"val_loss\":val_lo,\"train_acc\":ac,\"val_acc\":val_ac,\"Exceeding_ratio\":excess_ratio,\"Average_length_of_paragraph\":avg,\"Maximum_length_of_a_paragraph\":maxim,\"Average_length_of_sentences\":avg_sen_length,\"Maximum_length_of_a_sentence_in_a_paragraph\":n,\"Maximum_no_of_sentence_in_any_paragraph\":m,\"Vocabular_size\":vocab}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "\n",
        "original_ans=data_test_without_oos['tag'].tolist()\n",
        "\n",
        "output_class_pred=[]\n",
        "y_test=pred_without_oos\n",
        "y_test=y_test.tolist()\n",
        "for i in range(len(y_test)):\n",
        "    maxx=max(y_test[i])\n",
        "    output_class_pred.append(label_names[y_test[i].index(maxx)])\n",
        "cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "resi_without_oos=multiclass_metrics_without_oos(cnf_matrix,diff1,history_without_oos.history['loss'][-1],history_without_oos.history['val_loss'][-1],history_without_oos.history['accuracy'][-1],history_without_oos.history['val_accuracy'][-1])\n",
        "resi_without_oos.to_csv('results_text_without_oos.csv', mode='w', index = False, header=resi_without_oos.columns,columns=resi_without_oos.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HkeBwV382lhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_oos=get_model('binary_crossentropy','sigmoid')\n",
        "print(model_with_oos.summary())\n",
        "start = datetime.datetime.now()\n",
        "history_with_oos = model_with_oos.fit(train_cnn_data, y_train,  epochs=10,callbacks=callbacks_list,validation_split=0.1 )\n",
        "end = datetime.datetime.now()\n",
        "pred_with_oos=model_with_oos.predict(test_cnn_data_with_oos)\n",
        "# In[145]:\n",
        "\n",
        "diff2= (end - start)\n",
        "print('time taken by text_6 with_oos',diff2)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mGX5qgN12lhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_metric_with_oos_static_thresholding(y_test,original_ans):\n",
        "    output_class_pred=[]\n",
        "    y_test=y_test.tolist()\n",
        "    open_acc=[]#deep waaali accuracy which also checks ki classifier internal classes bhi sahi krta hai yaa nhi\n",
        "    pre=[]\n",
        "    recall=[]\n",
        "    f1=[]\n",
        "    specificity=[]\n",
        "    sensitivity=[]\n",
        "    GMean1=[]\n",
        "    Gmean2=[]\n",
        "    MCC=[]\n",
        "    bin_acc=[]\n",
        "    for i in range(len(y_test)):\n",
        "        ma=max(y_test[i])\n",
        "        output_class_pred.append([label_names[y_test[i].index(ma)],ma])\n",
        "    #print(y_test)\n",
        "    print(output_class_pred)\n",
        "    \n",
        "    \n",
        "        \n",
        "    for t in [0.6,0.7,0.8,0.9]: \n",
        "        for i in range(len(y_test)):\n",
        "            if(output_class_pred[i][1]<t):    ##t is threshold\n",
        "                output_class_pred[i][0]=\"oos\"\n",
        "        #print(output_class_pred)\n",
        "        \n",
        "        rightly_predicted_count=0\n",
        "        for i in range(len(y_test)):\n",
        "            if( output_class_pred[i][0]==original_ans[i]):\n",
        "                rightly_predicted_count+=1\n",
        "\n",
        "        #accuracy=rightly_predicted_count/len(y_test)\n",
        "        print('open_accuracy',rightly_predicted_count/len(y_test))\n",
        "        open_accuracy=rightly_predicted_count/len(y_test)\n",
        "        \n",
        "        TP=0\n",
        "        FP=0\n",
        "        TN=0\n",
        "        FN=0\n",
        "        df=[]\n",
        "        for i in range(len(y_test)):\n",
        "            if(  original_ans[i]!='oos' and output_class_pred[i][0]=='oos' ):\n",
        "                FP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]=='oos' ):\n",
        "                TP+=1\n",
        "                df.append(data_test_with_oos.loc[i,'title'])\n",
        "        \n",
        "            if(original_ans[i]==\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                FN+=1\n",
        "        \n",
        "            if(original_ans[i]!=\"oos\" and output_class_pred[i][0]!='oos' ):\n",
        "                TN+=1\n",
        "        print(\"TP->\",TP,\"FP->\",FP,\"TN->\",TN,\"FN->\",FN)\n",
        "        binary_acc=(TP+TN)/(TP+FP+FN+TN)\n",
        "        precision=TP/(TP+FP)\n",
        "        recalll=TP/(FN+TP)\n",
        "        F1=2*precision*recalll/(precision+recalll)\n",
        "        sensiti=TP/(TP+FN)\n",
        "        specifici=TN/(TN+FP)\n",
        "        numerator=TP*TN - FP*FN\n",
        "    \n",
        "        denominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "        MCc=numerator/denominator\n",
        "        G_mean1=np.sqrt(sensiti*precision)\n",
        "        G_mean2=np.sqrt(sensiti*specifici)\n",
        "        print(\"binary_accuracy\",binary_acc)\n",
        "        print('precision_oos:' ,TP/(TP+FP))\n",
        "        print('recall_oos:',TP/(FN+TP))\n",
        "        print(\"F1_oos:\",F1)\n",
        "        print(\"Specificity_oos:\",TN/(TN+FP))\n",
        "        print(\"Sensitivity_oos \",TP/(TP+FN))\n",
        "        print('G-mean1:',np.sqrt(sensiti*precision))\n",
        "        print(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "        print(\"MCC :\",MCc)\n",
        "        #open_acc.append(open_accuracy)\n",
        "        bin_acc.append(binary_acc)\n",
        "        open_acc.append(open_accuracy)\n",
        "        pre.append(precision)\n",
        "        recall.append(recalll)\n",
        "        f1.append(F1)\n",
        "        specificity.append(specifici)\n",
        "        sensitivity.append(sensiti)\n",
        "        GMean1.append(G_mean1)\n",
        "        Gmean2.append(G_mean2)\n",
        "        MCC.append(MCc)\n",
        "        \n",
        "    matrix=[0.6,0.7,0.8,0.9]\n",
        "    print(len(matrix))\n",
        "    data={'threshold':matrix,\"binary_accuracy\":bin_acc,'open_accuracy':open_acc,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC}\n",
        "    metric=pd.DataFrame(data)\n",
        "    return metric,df\n",
        "original_ans=data_test_with_oos['tag'].tolist()"
      ],
      "metadata": {
        "trusted": true,
        "id": "TF-yUGYT2lh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resi_with_oos_static_threshold,df=check_metric_with_oos_static_thresholding(pred_with_oos,original_ans)\n",
        "resi_with_oos_static_threshold.to_csv('results_text_with_oos_static_threshold.csv', mode='w', index = False, header=resi_with_oos_static_threshold.columns,columns=resi_with_oos_static_threshold.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "t64AHy0y2lh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "d7hFbdL82lh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resi_with_oos,df=check_metric_with_oos_dynamic_thresholding(pred_with_oos,original_ans)\n",
        "resi_with_oos.to_csv('results_text_with_oos_dynamic.csv', mode='w', index = False, header=resi_with_oos.columns,columns=resi_with_oos.columns)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RNJ9xVR_2lh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CLUSTERING"
      ],
      "metadata": {
        "id": "JkCfTLGW6OQb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:54:00.906488Z",
          "iopub.execute_input": "2022-01-12T13:54:00.906916Z",
          "iopub.status.idle": "2022-01-12T13:54:00.927391Z",
          "shell.execute_reply.started": "2022-01-12T13:54:00.906877Z",
          "shell.execute_reply": "2022-01-12T13:54:00.926128Z"
        },
        "trusted": true,
        "id": "7aYg11Kf2lh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clu=[111]*len(df)\n",
        "data_train=pd.DataFrame({'text':df,'clusters predicted by model(real)':clu})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:54:02.164053Z",
          "iopub.execute_input": "2022-01-12T13:54:02.164375Z",
          "iopub.status.idle": "2022-01-12T13:54:02.171555Z",
          "shell.execute_reply.started": "2022-01-12T13:54:02.164344Z",
          "shell.execute_reply": "2022-01-12T13:54:02.170518Z"
        },
        "trusted": true,
        "id": "N2bm0kW_2lh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:54:03.013852Z",
          "iopub.execute_input": "2022-01-12T13:54:03.014297Z",
          "iopub.status.idle": "2022-01-12T13:54:03.033284Z",
          "shell.execute_reply.started": "2022-01-12T13:54:03.014254Z",
          "shell.execute_reply": "2022-01-12T13:54:03.032512Z"
        },
        "trusted": true,
        "id": "Ntfby1yX2lh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "emb=1\n",
        "if (emb==1):\n",
        "    def vectorize_tfidf(text, maxx_features):\n",
        "    \n",
        "        vectorizer = TfidfVectorizer(max_features=maxx_features)\n",
        "        X = vectorizer.fit_transform(text)\n",
        "        '''scaler = MaxAbsScaler()\n",
        "        X = scaler.fit_transform(X)\n",
        "        '''\n",
        "        return X\n",
        "    text = data_train['text'].values\n",
        "    X_reduced= vectorize_tfidf(text, 2 ** 10)\n",
        "    \n",
        "\n",
        "elif(emb==2):\n",
        "    def vectorize_count(text, maxx_features=None):\n",
        "        count_vec = CountVectorizer()\n",
        "\n",
        "        count_train = count_vec.fit(text)\n",
        "        bag_of_words = count_vec.transform(text)\n",
        "        return bag_of_words\n",
        "\n",
        "    text=data_train['text'].values\n",
        "    X_reduced=vectorize_count(text)\n",
        "\n",
        "\n",
        "'''\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import abc\n",
        "#all_sentences\n",
        "\n",
        "all_sentences=data_train['text'].tolist()\n",
        "string=''\n",
        "for i in range(len(all_sentences)):\n",
        "    string+=all_sentences[i]\n",
        "    \n",
        "all_sentences = nltk.sent_tokenize(string)\n",
        "\n",
        "all_words = [nltk.word_tokenize(sent) for sent in all_sentences]\n",
        "\n",
        "\n",
        "#all_words = [nltk.word_tokenize(sent) for sent in [string]]\n",
        "model = gensim.models.Word2Vec(all_words)\n",
        "\n",
        "model.wv.vocab\n",
        "def vectorize_word2vec(data):\n",
        "    model = gensim.models.Word2Vec()\n",
        "    \n",
        "    \n",
        "import gensim \n",
        "from gensim.models import Word2Vec\n",
        "from gensim import models\n",
        "#model = gensim.models.KeyedVectors.load_word2vec_format(, binary=True)\n",
        "'''"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:54:03.796226Z",
          "iopub.execute_input": "2022-01-12T13:54:03.796552Z",
          "iopub.status.idle": "2022-01-12T13:54:03.829635Z",
          "shell.execute_reply.started": "2022-01-12T13:54:03.796522Z",
          "shell.execute_reply": "2022-01-12T13:54:03.828737Z"
        },
        "trusted": true,
        "id": "AXnlTjpm2lh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def make_embeddings(data,name):\n",
        "    if(name=='normal-message'):\n",
        "        print('sd')\n",
        "        model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "        print('ee')\n",
        "        X_reduced= model.encode(data['text'])\n",
        "    if name=='universal':\n",
        "        module_url = 'https://tfhub.dev/google/universal-sentence-encoder-large/5'\n",
        "        embed = hub.KerasLayer(module_url, trainable=True, name='USE_embedding')\n",
        "        return embed(data['text']).numpy()\n",
        "    return X_reduced\n",
        "\n",
        "X_reduced=make_embeddings(data_train,'universal')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:54:19.00721Z",
          "iopub.execute_input": "2022-01-12T13:54:19.007527Z",
          "iopub.status.idle": "2022-01-12T13:54:41.070492Z",
          "shell.execute_reply.started": "2022-01-12T13:54:19.007496Z",
          "shell.execute_reply": "2022-01-12T13:54:41.069637Z"
        },
        "trusted": true,
        "id": "KM702TcR2lh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neighborhood_search(emb):\n",
        "    \n",
        "    index = faiss.IndexFlatIP(emb.shape[1])\n",
        "    faiss.normalize_L2(emb)\n",
        "    index.add(emb)\n",
        "    faiss.write_index(index,'yodd')\n",
        "    \n",
        "    sim, I = index.search(emb, 15)\n",
        "    \n",
        "        \n",
        "    return I,sim\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "def blend_neighborhood(emb, match_index_lst, similarities_lst):\n",
        "    new_emb = emb.copy()\n",
        "    for i in range(emb.shape[0]):\n",
        "        cur_emb = emb[match_index_lst[i]]\n",
        "        #print(cur_emb)\n",
        "        weights = np.expand_dims(similarities_lst[i], 1)\n",
        "        #print(weights)\n",
        "        new_emb[i] = (cur_emb * weights).sum(axis=0)\n",
        "        #print(new_emb[i])\n",
        "    new_emb = normalize(new_emb, axis=1)\n",
        "    #print(weights)\n",
        "    #print(new_emb[199])\n",
        "    #print(new_emb)\n",
        "    return new_emb\n",
        "\n",
        "\n",
        "threshes=[0.8,0.9]#must be a list of thresholds kyunki iteratively similarities badhti jaani  chahiye\n",
        "def iterative_neighborhood_blending(emb, threshes):\n",
        "    for thresh in threshes:\n",
        "        match_index_lst, similarities_lst = neighborhood_search(emb, thresh)\n",
        "        emb = blend_neighborhood(emb, match_index_lst, similarities_lst)\n",
        "    return match_index_lst,similarities_lst\n",
        "match_index_lst,similarities_lst=neighborhood_search(X_reduced)\n",
        "X_reduced= blend_neighborhood(X_reduced, match_index_lst, similarities_lst)\n",
        "#match_index_lst,similarities_lst=iterative_neighborhood_blending(X_reduced, threshes)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-10T16:05:34.512731Z",
          "iopub.execute_input": "2022-01-10T16:05:34.513017Z",
          "iopub.status.idle": "2022-01-10T16:05:34.552622Z",
          "shell.execute_reply.started": "2022-01-10T16:05:34.512989Z",
          "shell.execute_reply": "2022-01-10T16:05:34.551815Z"
        },
        "trusted": true,
        "id": "TOP4oHzg2lh6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y26pj7G42lh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data_train['text'][4])"
      ],
      "metadata": {
        "trusted": true,
        "id": "goQw_0aL2lh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-10T16:05:34.554Z",
          "iopub.execute_input": "2022-01-10T16:05:34.554355Z",
          "iopub.status.idle": "2022-01-10T16:05:34.559705Z",
          "shell.execute_reply.started": "2022-01-10T16:05:34.554319Z",
          "shell.execute_reply": "2022-01-10T16:05:34.558445Z"
        },
        "trusted": true,
        "id": "f3LFZLGV2lh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "distortions = []\n",
        "K = range(1,20)\n",
        "for k in K:\n",
        "    kmeanModel = KMeans(n_clusters=k)\n",
        "    kmeanModel.fit(X_reduced)\n",
        "    distortions.append(kmeanModel.inertia_)\n",
        "    \n",
        "plt.figure(figsize=(16,8))\n",
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('k')\n",
        "plt.ylabel('Distortion')\n",
        "plt.title('The Elbow Method showing the optimal k')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "BnAogBa92lh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def find_optimal_clusters(data, max_k):\n",
        "    iters = range(2, max_k+1, )\n",
        "    \n",
        "    sse = []\n",
        "    for k in iters:\n",
        "        sse.append(MiniBatchKMeans(n_clusters=k, random_state=20).fit(data).inertia_)\n",
        "        print('Fit {} clusters'.format(k))\n",
        "        \n",
        "    f, ax = plt.subplots(1, 1)\n",
        "    ax.plot(iters, sse, marker='o')\n",
        "    ax.set_xlabel('Cluster Centers')\n",
        "    ax.set_xticks(iters)\n",
        "    ax.set_xticklabels(iters)\n",
        "    ax.set_ylabel('SSE')\n",
        "    ax.set_title('SSE by Cluster Center Plot')\n",
        "    return sse\n",
        "    \n",
        "sse=find_optimal_clusters(X_reduced, 10)"
      ],
      "metadata": {
        "trusted": true,
        "id": "2RcKcDj_2lh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/arvkevi/kneed.git"
      ],
      "metadata": {
        "trusted": true,
        "id": "tJBKe3Tg2lh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "x = range(2,11)\n",
        "y = sse\n",
        "\n",
        "kn = KneeLocator(\n",
        "    x,\n",
        "    y,\n",
        "    curve='convex',\n",
        "    direction='decreasing',\n",
        "    interp_method='interp1d',\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.xticks(range(1,10))\n",
        "plt.plot(x, y, 'bx-')\n",
        "#plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
        "\n",
        "print(kn.knee)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZXw6L0C_2lh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kn = KneeLocator(\n",
        "    x,\n",
        "    y,\n",
        "    curve='convex',\n",
        "    direction='decreasing',\n",
        "    interp_method='polynomial',\n",
        "    online=True\n",
        ")\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('f(x)')\n",
        "plt.xticks(range(1,21))\n",
        "plt.plot(x, y, 'bx-')\n",
        "#plt.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n",
        "print(kn.knee)"
      ],
      "metadata": {
        "trusted": true,
        "id": "KCs3hLhx2lh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wDR7mGa72lh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "def Best_Clustering(data , max_clusters = int(np.sqrt(len(data_train))), scaling = True, visualization = True):\n",
        "    \n",
        "    n_clusters_list=[]\n",
        "    silhouette_list=[]\n",
        "    \n",
        "    if scaling:\n",
        "    #Data Scaling\n",
        "        scaler = MinMaxScaler()\n",
        "        data_std = scaler.fit_transform(data)\n",
        "    else:    \n",
        "        data_std = data\n",
        "        \n",
        "    for n_c in range(2,max_clusters+1): \n",
        "        kmeans_model=MiniBatchKMeans(n_clusters=n_c, random_state=20).fit(data_std)\n",
        "        '''kmeans_model = KMeans(n_clusters=n_c, random_state=42).fit(data_std)'''\n",
        "        labels = kmeans_model.labels_\n",
        "        n_clusters_list.append(n_c)\n",
        "        silhouette_list.append(silhouette_score(data_std, labels, metric='euclidean'))\n",
        "    \n",
        "    # Best Parameters\n",
        "    param1 = n_clusters_list[np.argmax(silhouette_list)]\n",
        "    param2 = max(silhouette_list)\n",
        "    best_params = param1,param2\n",
        "    \n",
        "    # Data labeling with the best model\n",
        "    kmeans_best=MiniBatchKMeans(n_clusters=param1,random_state=20).fit(data_std)\n",
        "    '''kmeans_best = KMeans(n_clusters= param1 , random_state=42).fit(data_std)'''\n",
        "    labels_best = kmeans_best.labels_\n",
        "    print(labels_best)\n",
        "    print(data.shape)\n",
        "    #labeled_data = np.concatenate((data,labels_best.reshape(-1,1)),axis=1)\n",
        "        \n",
        "    if visualization:\n",
        "        fig = plt.figure(figsize=(12,6))\n",
        "        ax = fig.add_subplot(111)\n",
        "        ax.plot(n_clusters_list,silhouette_list, linewidth=3,\n",
        "                label = \"Silhouette Score Against # of Clusters\")\n",
        "        ax.set_xlabel(\"Number of clusters\")\n",
        "        ax.set_ylabel(\"Silhouette score\")\n",
        "        ax.set_title('Silhouette score according to number of clusters')\n",
        "        ax.grid(True)\n",
        "        plt.plot(param1,param2, \"tomato\", marker=\"*\",\n",
        "             markersize=20, label = 'Best Silhouette Score')\n",
        "    \n",
        "        plt.legend(loc=\"best\",fontsize = 'large')\n",
        "        plt.show();\n",
        "        print( \" Best Clustering corresponds to the following point : \\\n",
        "        Number of clusters = %i & Silhouette_score = %.2f.\"%best_params) \n",
        "    else:\n",
        "        return best_params, labeled_data\n",
        "Best_Clustering(data=X_reduced, scaling = False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "m1mCQE5e2lh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "plt.figure(figsize =(8, 8))\n",
        "plt.title('Visualising the data')\n",
        "Dendrogram = shc.dendrogram((shc.linkage((X_reduced.toarray()), method ='ward')))"
      ],
      "metadata": {
        "trusted": true,
        "id": "No4KLgD22lh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ac3 = AgglomerativeClustering(n_clusters = 3)\n",
        "  \n",
        "# Visualizing the clustering\n",
        "plt.figure(figsize =(6, 6))\n",
        "plt.scatter(X_reduced['P1'], X_reduced['P2'], \n",
        "           c = ac3.fit_predict(X_reduced), cmap ='rainbow')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "hJtAQ1wY2lh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "silhouette_scores = []\n",
        "for k in range(2,20):\n",
        "    ac=AgglomerativeClustering(n_clusters = k)\n",
        "    \n",
        "    silhouette_scores.append(\n",
        "        silhouette_score(X_reduced.toarray(), ac.fit_predict(X_reduced.toarray())))\n",
        "\n",
        "# Plotting a bar graph to compare the results\n",
        "K=range(2,20)\n",
        "plt.bar(K, silhouette_scores)\n",
        "plt.xlabel('Number of clusters', fontsize = 20)\n",
        "plt.ylabel('S(i)', fontsize = 20)\n",
        "plt.show()\n",
        "    \n",
        "   "
      ],
      "metadata": {
        "trusted": true,
        "id": "6dj4gamw2liA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "n_optimum_clusters=int(input('enter the number of optimum clusters'))##########################IMPORTANTTTTT\n",
        "\n",
        "\n",
        "\n",
        "def kmeans_clustering(X_reduced):\n",
        "    kmeans = KMeans(n_clusters=n_optimum_clusters)\n",
        "    kmeans.fit(X_reduced)\n",
        "    print(pd.Series(kmeans.labels_).value_counts())\n",
        "    return kmeans\n",
        "kmeans=kmeans_clustering(X_reduced)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:54:58.433316Z",
          "iopub.execute_input": "2022-01-12T13:54:58.433641Z",
          "iopub.status.idle": "2022-01-12T13:55:01.523278Z",
          "shell.execute_reply.started": "2022-01-12T13:54:58.433611Z",
          "shell.execute_reply": "2022-01-12T13:55:01.522173Z"
        },
        "trusted": true,
        "id": "t90rtS0Q2liG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df1.copy()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:06.523731Z",
          "iopub.execute_input": "2022-01-12T13:55:06.524079Z",
          "iopub.status.idle": "2022-01-12T13:55:06.528158Z",
          "shell.execute_reply.started": "2022-01-12T13:55:06.52405Z",
          "shell.execute_reply": "2022-01-12T13:55:06.527063Z"
        },
        "trusted": true,
        "id": "4SBccCJG2liH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:06.668666Z",
          "iopub.execute_input": "2022-01-12T13:55:06.669115Z",
          "iopub.status.idle": "2022-01-12T13:55:06.680668Z",
          "shell.execute_reply.started": "2022-01-12T13:55:06.669082Z",
          "shell.execute_reply": "2022-01-12T13:55:06.679869Z"
        },
        "trusted": true,
        "id": "9tRAAB4V2liH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:07.05786Z",
          "iopub.execute_input": "2022-01-12T13:55:07.058242Z",
          "iopub.status.idle": "2022-01-12T13:55:07.070603Z",
          "shell.execute_reply.started": "2022-01-12T13:55:07.058209Z",
          "shell.execute_reply": "2022-01-12T13:55:07.069692Z"
        },
        "trusted": true,
        "id": "2MG28c8a2liH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=data_train['text'].tolist()\n",
        "data_train['REALLABEL']=['j']*len(data_train)\n",
        "for i in f:\n",
        "    d=df['title'].tolist()\n",
        "    if(i in d ):\n",
        "        ind=d.index(i)\n",
        "        data_train['REALLABEL'][f.index(i)]=df['tag'][ind]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:07.95745Z",
          "iopub.execute_input": "2022-01-12T13:55:07.957806Z",
          "iopub.status.idle": "2022-01-12T13:55:08.060395Z",
          "shell.execute_reply.started": "2022-01-12T13:55:07.957747Z",
          "shell.execute_reply": "2022-01-12T13:55:08.059531Z"
        },
        "trusted": true,
        "id": "184AF0kD2liH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:08.891436Z",
          "iopub.execute_input": "2022-01-12T13:55:08.891933Z",
          "iopub.status.idle": "2022-01-12T13:55:08.91488Z",
          "shell.execute_reply.started": "2022-01-12T13:55:08.891884Z",
          "shell.execute_reply": "2022-01-12T13:55:08.914125Z"
        },
        "trusted": true,
        "id": "78389D7O2liH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_post_info_by_cluster(number, \n",
        "                             data,\n",
        "                             cluster):\n",
        "    \n",
        "    print(data[cluster.labels_ == number])\n",
        "    \n",
        "    return(data[cluster.labels_ == number]['REALLABEL'])\n",
        "for i in range(n_optimum_clusters):\n",
        "    \n",
        "    print(f\"Cluster {i}:\\n\")\n",
        "    print(get_post_info_by_cluster(i, data = data_train,cluster = kmeans))\n",
        "    \n",
        "    #print(get_post_info_by_cluster(i, \n",
        "                                   #data = data_train['text'],\n",
        "                                   #cluster = kmeans))\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:09.975519Z",
          "iopub.execute_input": "2022-01-12T13:55:09.975874Z",
          "iopub.status.idle": "2022-01-12T13:55:09.996616Z",
          "shell.execute_reply.started": "2022-01-12T13:55:09.975836Z",
          "shell.execute_reply": "2022-01-12T13:55:09.995539Z"
        },
        "trusted": true,
        "id": "FzJjEoxt2liI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r=kmeans.labels_"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:11.006292Z",
          "iopub.execute_input": "2022-01-12T13:55:11.006627Z",
          "iopub.status.idle": "2022-01-12T13:55:11.01084Z",
          "shell.execute_reply.started": "2022-01-12T13:55:11.006597Z",
          "shell.execute_reply": "2022-01-12T13:55:11.009954Z"
        },
        "trusted": true,
        "id": "PcBk1ooF2liI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(data_train)):\n",
        "    data_train['clusters predicted by model(real)'][i]='cluster'+str(r[i])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:12.355339Z",
          "iopub.execute_input": "2022-01-12T13:55:12.355656Z",
          "iopub.status.idle": "2022-01-12T13:55:12.418159Z",
          "shell.execute_reply.started": "2022-01-12T13:55:12.355627Z",
          "shell.execute_reply": "2022-01-12T13:55:12.417276Z"
        },
        "trusted": true,
        "id": "9W4Wauo32liI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:55:12.810246Z",
          "iopub.execute_input": "2022-01-12T13:55:12.81057Z",
          "iopub.status.idle": "2022-01-12T13:55:12.823804Z",
          "shell.execute_reply.started": "2022-01-12T13:55:12.81054Z",
          "shell.execute_reply": "2022-01-12T13:55:12.822939Z"
        },
        "trusted": true,
        "id": "pg7GCjFE2liI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_keywords(cluster,no_of_clusters):\n",
        "    tf=[]\n",
        "    yake=[]\n",
        "    textrank=[]\n",
        "    singlerank=[]\n",
        "    topicrank=[]\n",
        "    topicalpagerank=[]\n",
        "    positionrank=[]\n",
        "    multipartite=[]\n",
        "    kea=[]\n",
        "    wingnus=[]\n",
        "    keybert=[]\n",
        "    for i in range(no_of_clusters):\n",
        "        df=data_train['text'][cluster.labels_==i]\n",
        "        df=df.to_frame()\n",
        "        df=df.reset_index()\n",
        "        textt=\"\"\n",
        "        \n",
        "        for j in range(len(df)):\n",
        "            textt+=df['text'][j]\n",
        "        \n",
        "        extractor = pke.unsupervised.TfIdf() \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        tf.append(keyphrases)\n",
        "        \n",
        "        extractor = pke.unsupervised.YAKE() \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        yake.append(keyphrases)\n",
        "        \n",
        "        extractor = pke.unsupervised.TextRank () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        textrank.append(keyphrases)\n",
        "        \n",
        "        extractor = pke.unsupervised.SingleRank () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        singlerank.append(keyphrases)\n",
        "        \n",
        "        extractor = pke.unsupervised.TopicRank () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=2, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        topicrank.append(keyphrases)\n",
        "        \n",
        "        extractor = pke.unsupervised.TopicalPageRank () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=2, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        topicalpagerank.append(keyphrases)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        extractor = pke.unsupervised.MultipartiteRank () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=2, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        multipartite.append(keyphrases)\n",
        "        \n",
        "        \n",
        "        extractor = pke.supervised.Kea () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        kea.append(keyphrases)\n",
        "        \n",
        "        extractor = pke.supervised.WINGNUS () \n",
        "        extractor.load_document(textt)\n",
        "        extractor.candidate_selection()\n",
        "        extractor.candidate_weighting()\n",
        "        keyphrases = extractor.get_n_best(n=5, stemming=False)\n",
        "        print(\"Cluster\", i)\n",
        "        #print(keyphrases,\"\\n\")\n",
        "        wingnus.append(keyphrases)\n",
        "        \n",
        "        kw_model = KeyBERT()\n",
        "        KeyBert=kw_model.extract_keywords(textt, keyphrase_ngram_range=(1, 2), stop_words=None)\n",
        "        keybert.append(KeyBert)\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "    data = {'TFIDF':tf,'Yake':yake,\"TextRank\":textrank,\"SingleRank\":singlerank,\"kea\":kea,\"WINGNUS\":wingnus,\"Keybert\":keybert,}\n",
        "    #data = {\"Keybert\":keybert,}\n",
        "    \n",
        "    key=pd.DataFrame(data)\n",
        "    return key\n",
        "        \n",
        "\n",
        "key=get_keywords(kmeans,n_optimum_clusters)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:56:13.240051Z",
          "iopub.execute_input": "2022-01-12T13:56:13.240402Z",
          "iopub.status.idle": "2022-01-12T13:56:36.139253Z",
          "shell.execute_reply.started": "2022-01-12T13:56:13.24037Z",
          "shell.execute_reply": "2022-01-12T13:56:36.138205Z"
        },
        "trusted": true,
        "id": "ZxsI1bnb2liJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "key.to_csv('keyy.csv',mode='a', index = False, header=key.columns,columns=key.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:56:36.140943Z",
          "iopub.execute_input": "2022-01-12T13:56:36.141269Z",
          "iopub.status.idle": "2022-01-12T13:56:36.151125Z",
          "shell.execute_reply.started": "2022-01-12T13:56:36.141235Z",
          "shell.execute_reply": "2022-01-12T13:56:36.150135Z"
        },
        "trusted": true,
        "id": "tgCwupOi2liJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install https://github.com/MartinoMensio/spacy-universal-sentence-encoder-tfhub/releases/download/en_use_md-0.2.0/en_use_md-0.2.0.tar.gz#en_use_md-0.2.0"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:56:36.15309Z",
          "iopub.execute_input": "2022-01-12T13:56:36.153508Z",
          "iopub.status.idle": "2022-01-12T13:57:56.378616Z",
          "shell.execute_reply.started": "2022-01-12T13:56:36.153465Z",
          "shell.execute_reply": "2022-01-12T13:57:56.377597Z"
        },
        "trusted": true,
        "id": "dYS-pgh-2liJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train['REALLABEL'][19]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:58:27.966002Z",
          "iopub.execute_input": "2022-01-12T13:58:27.966327Z",
          "iopub.status.idle": "2022-01-12T13:58:27.97297Z",
          "shell.execute_reply.started": "2022-01-12T13:58:27.966292Z",
          "shell.execute_reply": "2022-01-12T13:58:27.971848Z"
        },
        "trusted": true,
        "id": "YX6Vvm0r2liK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using method tfidf\n",
        "\n",
        "nlp=spacy.load('en_core_web_lg')\n",
        "print(nlp('BookRestaurant').similarity(nlp('restaurant')))\n",
        "topx=5\n",
        "clusters=n_optimum_clusters\n",
        "print(clusters)\n",
        "average=[]\n",
        "l=[]\n",
        "tt=0\n",
        "#methods=[\"TFIDF\",'Yake',\"TextRank\",\"SingleRank\",\"kea\",\"WINGNUS\",'Keybert']\n",
        "methods=['Keybert']\n",
        "for metho in methods:\n",
        "    cl=[]\n",
        "    for i in range(len(data_train)):\n",
        "        print(tt)\n",
        "        tt+=1\n",
        "        \n",
        "        for j in range(clusters):\n",
        "        \n",
        "            for top in range(topx):\n",
        "                if(data_train['REALLABEL'][i]=='BookRestaurant'):\n",
        "                    data_train['REALLABEL'][i]='Book Restaurant'\n",
        "                elif(data_train['REALLABEL'][i]=='GetWeather'):\n",
        "                    data_train['REALLABEL'][i]='Get Weather'\n",
        "                elif(data_train['REALLABEL'][i]=='SearchScreeningEvent'):\n",
        "                    data_train['REALLABEL'][i]='Search Screening Event'\n",
        "                elif(data_train['REALLABEL'][i]=='PlayMusic'):\n",
        "                    data_train['REALLABEL'][i]='Play Music'\n",
        "                elif(data_train['REALLABEL'][i]=='AddToPlaylist'):\n",
        "                    data_train['REALLABEL'][i]='Add To Playlist'\n",
        "                elif(data_train['REALLABEL'][i]=='RateBook'):\n",
        "                    data_train['REALLABEL'][i]='Rate Book'\n",
        "                elif(data_train['REALLABEL'][i]=='SearchCreativeWork'):\n",
        "                    data_train['REALLABEL'][i]='Search Creative Work'\n",
        "                \n",
        "                \n",
        "                \n",
        "                \n",
        "                score=(nlp(data_train['REALLABEL'][i])).similarity(nlp(key[metho][j][top][0]))\n",
        "                #print(type(key[metho][j][top][0]))\n",
        "                #print(nlp(data_train['REALLABEL'][i]))\n",
        "                print(score)\n",
        "                l.append(score)\n",
        "                \n",
        "        #print(sum(l))\n",
        "            try:\n",
        "                av=sum(l)/len(l)\n",
        "                #print(av)\n",
        "                average.append(av)\n",
        "            except:\n",
        "                pass\n",
        "            l=[]\n",
        "        \n",
        "        maxxi=max(average)\n",
        "        ind=average.index(maxxi)\n",
        "        #print(ind)\n",
        "        average=[]\n",
        "        #na='clusters based on similarity score'+metho\n",
        "        su='cluster'+str(ind)\n",
        "        cl.append(su)\n",
        "        \n",
        "        #data_train[na][i]='cluster'+str(ind)\n",
        "    na='clusters based on similarity score'+metho\n",
        "    data_train[na]=cl\n",
        "    cl=[]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:58:29.135017Z",
          "iopub.execute_input": "2022-01-12T13:58:29.135345Z",
          "iopub.status.idle": "2022-01-12T13:59:57.662594Z",
          "shell.execute_reply.started": "2022-01-12T13:58:29.135312Z",
          "shell.execute_reply": "2022-01-12T13:59:57.661721Z"
        },
        "trusted": true,
        "id": "nf6Mx7Nm2liK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:59:57.664975Z",
          "iopub.execute_input": "2022-01-12T13:59:57.665307Z",
          "iopub.status.idle": "2022-01-12T13:59:57.682535Z",
          "shell.execute_reply.started": "2022-01-12T13:59:57.665277Z",
          "shell.execute_reply": "2022-01-12T13:59:57.681695Z"
        },
        "trusted": true,
        "id": "FrrC4txM2liK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multiclass_metrics(cnf_matrix):\n",
        "\tcnf_matrix=np.asarray(cnf_matrix)\n",
        "\tFP = cnf_matrix.sum(axis=0) - np.diag(cnf_matrix) \n",
        "\tFN = cnf_matrix.sum(axis=1) - np.diag(cnf_matrix)\n",
        "\tTP = np.diag(cnf_matrix)\n",
        "\tTN = cnf_matrix.sum() - (FP + FN + TP)\n",
        "\tFP = FP.astype(float)\n",
        "\tFN = FN.astype(float)\n",
        "\tTP = TP.astype(float)\n",
        "\tTN = TN.astype(float)\n",
        "\n",
        "\tTP=np.sum(TP)\n",
        "\tTN=np.sum(TN)\n",
        "\tFP=np.sum(FP)\n",
        "\tFN=np.sum(FN)\n",
        "\n",
        "\n",
        "\taccuracy=(TP+TN)/(TP+FP+FN+TN)\n",
        "\tprecision=TP/(TP+FP)\n",
        "\trecalll=TP/(FN+TP)\n",
        "\tF1=2*precision*recalll/(precision+recalll)\n",
        "\tsensiti=TP/(TP+FN)\n",
        "\tspecifici=TN/(TN+FP)\n",
        "\tnumerator=TP*TN - FP*FN\n",
        "\tnmi=normalized_mutual_info_score(original_ans,output_class_pred)\n",
        "\tars=adjusted_rand_score(original_ans,output_class_pred)\n",
        "\trand_scor=rand_score(original_ans,output_class_pred)\n",
        "\tfms=fowlkes_mallows_score(original_ans,output_class_pred)\n",
        "    \n",
        "\tdenominator=np.sqrt((TP+FP)*(FN+TN)*(FP+TN)* (TP+FN))\n",
        "\tMCc=numerator/denominator\n",
        "\tG_mean1=np.sqrt(sensiti*precision)\n",
        "\tG_mean2=np.sqrt(sensiti*specifici)\n",
        "\tprint('NMI',nmi)\n",
        "\tprint('ARS',ars)\n",
        "\tprint('rand_score',rand_scor)\n",
        "\tprint('FMS',fms)\n",
        "\tprint('accuracy',accuracy)\n",
        "\tprint('precision:' ,TP/(TP+FP))\n",
        "\tprint('recall:',TP/(FN+TP))\n",
        "\tprint(\"F1:\",F1)\n",
        "\tprint(\"Specificity:\",TN/(TN+FP))\n",
        "\tprint(\"Sensitivity \",TP/(TP+FN))\n",
        "\tprint('G-mean1:',np.sqrt(sensiti*precision))\n",
        "\tprint(\"G-mean2\",np.sqrt(sensiti*specifici))\n",
        "\tprint(\"MCC :\",MCc)\n",
        "\tacc=[]\n",
        "\tpre=[]\n",
        "\trecall=[]\n",
        "\tf1=[]\n",
        "\tspecificity=[]\n",
        "\tsensitivity=[]\n",
        "\tGMean1=[]\n",
        "\tGmean2=[]\n",
        "\tMCC=[]\n",
        "\ttp=[]\n",
        "\tfp=[]\n",
        "\tfn=[]\n",
        "\ttn=[]\n",
        "\tacc.append(accuracy)\n",
        "\tpre.append(precision)\n",
        "\trecall.append(recalll)\n",
        "\tf1.append(F1)\n",
        "\tspecificity.append(specifici)\n",
        "\tsensitivity.append(sensiti)\n",
        "\tGMean1.append(G_mean1)\n",
        "\tGmean2.append(G_mean2)\n",
        "\tMCC.append(MCc)\n",
        "\ttp.append(TP)\n",
        "\tfp.append(FP)\n",
        "\ttn.append(TN)\n",
        "\tfn.append(FN)\n",
        "\tdata={'accuracy':acc,'ARS':ars,'rand_score':rand_scor,'NMI':nmi,'FMS':fms,\"precision\":pre,'recall':recall,'F1_score':f1,'specificity':specificity,'sensitivity':sensitivity,'Gmean1':GMean1,\"Gmean2\":Gmean2,\"MCC\":MCC,\"TP\":tp,\"FP\":fp,\"TN\":tn,\"FN\":fn}\n",
        "\tmetric=pd.DataFrame(data)\n",
        "\treturn metric\n",
        "import time, datetime\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "#original_ans=data_test_without_oos['tag'].tolist()\n",
        "\n",
        "output_class_pred=[]\n",
        "#y_test=pred_without_oos\n",
        "#y_test=y_test.tolist()\n",
        "#for i in range(len(y_test)):\n",
        "    #maxx=max(y_test[i])\n",
        "    #output_class_pred.append(label_names[y_test[i].index(maxx)])\n",
        "\n",
        "for me in methods:\n",
        "    original_ans=data_train['clusters predicted by model(real)'].tolist()\n",
        "    name_of_method='clusters based on similarity score'+me\n",
        "    output_class_pred=data_train[name_of_method].tolist()\n",
        "    cnf_matrix=confusion_matrix(original_ans,output_class_pred)\n",
        "    \n",
        "    from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "    from sklearn.metrics.cluster import v_measure_score\n",
        "    from sklearn.metrics.cluster import adjusted_rand_score\n",
        "    from sklearn.metrics.cluster import rand_score\n",
        "    from sklearn.metrics.cluster import fowlkes_mallows_score\n",
        "    #print(normalized_mutual_info_score(original_ans,output_class_pred))\n",
        "    print(cnf_matrix)\n",
        "    resi=multiclass_metrics(cnf_matrix)\n",
        "    resi.to_csv('results_text_label.csv', mode='a', index = False, header=resi.columns,columns=resi.columns)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2022-01-12T13:59:57.684154Z",
          "iopub.execute_input": "2022-01-12T13:59:57.684503Z",
          "iopub.status.idle": "2022-01-12T13:59:57.730239Z",
          "shell.execute_reply.started": "2022-01-12T13:59:57.684475Z",
          "shell.execute_reply": "2022-01-12T13:59:57.729449Z"
        },
        "trusted": true,
        "id": "LoqDu1WB2liL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5xhZ31D92liL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}